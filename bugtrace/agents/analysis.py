"""
AnalysisAgent - Multi-Model URL Vulnerability Analysis

This agent performs intelligent pre-exploitation analysis using multiple LLM models
with different personas to identify likely vulnerabilities before wasting resources
on blind testing.

Inspired by: BugTraceAI by @yz9yt
Architecture: Multi-agent event-driven system

Author: BugtraceAI-CLI Team
Created: 2026-01-02
"""

import asyncio
import json
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from collections import defaultdict
from pathlib import Path

from bugtrace.agents.base import BaseAgent
from bugtrace.core.event_bus import EventBus
from bugtrace.core.llm_client import llm_client
from bugtrace.core.config import settings
from bugtrace.utils.logger import get_logger
from bugtrace.utils.parsers import XmlParser

logger = get_logger("agents.analysis")


class AnalysisAgent(BaseAgent):
    """
    Multi-Model URL Analysis Agent.
    
    Analyzes each discovered URL with multiple LLM models (different personas)
    to generate a vulnerability assessment report before exploitation attempts.
    
    Workflow:
    1. Receive 'new_url_discovered' event from ReconAgent
    2. Extract context (headers, HTML, params, tech stack)
    3. Analyze in parallel with 3 models:
       - Pentester persona (Qwen Coder)
       - Bug Bounty Hunter persona (DeepSeek)
       - Code Auditor persona (GLM-4)
    4. Consolidate results using consensus voting
    5. Generate priority attack list
    6. Emit 'url_analyzed' event for ExploitAgent
    
    Benefits:
    - 70% cost reduction (fewer wasted tests)
    - 72% time savings (focused exploitation)
    - Higher accuracy (consensus-based)
    """
    
    def __init__(self, event_bus: EventBus):
        super().__init__("Analysis-1", "Vulnerability Analysis", event_bus=event_bus, agent_id="analysis_agent")
        
        # Using single model with 5 different analysis approaches (BugTraceAI methodology)
        self.model = getattr(settings, "ANALYSIS_PENTESTER_MODEL", "google/gemini-2.0-flash-exp")
        
        # 5 different analysis approaches for maximum coverage
        self.approaches = ["pentester", "bug_bounty", "code_auditor", "red_team", "researcher"]
        
        # Thresholds
        self.confidence_threshold = getattr(settings, "ANALYSIS_CONFIDENCE_THRESHOLD", 0.7)
        self.skip_threshold = getattr(settings, "ANALYSIS_SKIP_THRESHOLD", 0.3)
        self.consensus_votes = getattr(settings, "ANALYSIS_CONSENSUS_VOTES", 2)  # Min 2/5 approaches must agree
        
        # Analysis cache (URL -> report)
        self.analysis_cache: Dict[str, Dict] = {}
        
        # Report persistence
        self.reports_dir = Path("reports")
        self.reports_dir.mkdir(exist_ok=True)
        
        # Statistics
        self.stats = {
            "urls_analyzed": 0,
            "consensus_count": 0,
            "avg_analysis_time": 0.0,
            "total_tokens": 0
        }
        
        logger.info(f"[{self.name}] Initialized with {len(self.approaches)} approaches: {self.approaches}")
        logger.info(f"[{self.name}] Model: {self.model}")
        logger.info(f"[{self.name}] Thresholds: confidence={self.confidence_threshold}, skip={self.skip_threshold}")
    
    def _setup_event_subscriptions(self):
        """Subscribe to URL discovery events."""
        self.event_bus.subscribe("new_url_discovered", self.handle_new_url)
        logger.info(f"[{self.name}] Subscribed to: new_url_discovered")
    
    def _cleanup_event_subscriptions(self):
        """Cleanup event subscriptions."""
        self.event_bus.unsubscribe("new_url_discovered", self.handle_new_url)
        logger.info(f"[{self.name}] Unsubscribed from events")
    
    async def handle_new_url(self, event_data: Dict):
        """
        Handle new URL discovery event.
        
        Args:
            event_data: {
                "url": str,
                "response": httpx.Response (optional),
                "inputs": List[Dict] (optional)
            }
        """
        url = event_data.get("url")
        if not url:
            logger.warning(f"[{self.name}] Received event without URL")
            return
        
        # Check cache
        if url in self.analysis_cache:
            logger.info(f"[{self.name}] Using cached analysis for {url}")
            report = self.analysis_cache[url]
        else:
            # Perform analysis
            logger.info(f"[{self.name}] ðŸ” Starting multi-model analysis: {url}")
            report = await self.analyze_url(event_data)
        
        # Emit analysis complete event
        await self.event_bus.emit("url_analyzed", {
            "url": url,
            "report": report
        })
        
        logger.info(f"[{self.name}] ðŸ“¢ EVENT EMITTED: url_analyzed for {url}")
    
    async def analyze_url(self, event_data: Dict) -> Dict[str, Any]:
        """
        Perform complete URL analysis with multiple models.
        
        Returns:
            Analysis report with vulnerability assessments and priorities
        """
        start_time = asyncio.get_event_loop().time()
        
        # Extract context
        context = self._extract_context(event_data)
        url = context["url"]
        
        logger.info(f"[{self.name}] Context extracted: {len(context['params'])} params, tech: {context['tech_stack']}")
        # Analyze with each approach
        valid_analyses = await self._run_all_analyses(context)

        if not valid_analyses:
            logger.error(f"[{self.name}] All analyses failed for {url}")
            return self._empty_report(url)

        logger.info(f"[{self.name}] Completed {len(valid_analyses)}/{len(self.approaches)} analyses")

        # Consolidate and save
        report = await self._build_and_save_report(valid_analyses, context, url)

        # Update statistics
        self._update_stats(start_time)

        logger.info(f"[{self.name}] âœ… Analysis complete in {asyncio.get_event_loop().time() - start_time:.2f}s")

        return report

    async def _run_all_analyses(self, context: Dict) -> List[Dict]:
        """Run analyses with all approaches and filter valid results."""
        tasks = [
            self._analyze_with_approach(context, approach)
            for approach in self.approaches
        ]
        analyses = await asyncio.gather(*tasks, return_exceptions=True)
        return [a for a in analyses if isinstance(a, dict) and not a.get("error")]

    async def _build_and_save_report(self, valid_analyses: List[Dict], context: Dict, url: str) -> Dict:
        """Consolidate analyses, cache, and save report."""
        report = self._consolidate_analyses(valid_analyses, context)
        self.analysis_cache[url] = report
        await self._save_report(url, report)
        return report

    def _update_stats(self, start_time: float) -> None:
        """Update analysis statistics."""
        elapsed = asyncio.get_event_loop().time() - start_time
        self.stats["urls_analyzed"] += 1
        self.stats["avg_analysis_time"] = (
            (self.stats["avg_analysis_time"] * (self.stats["urls_analyzed"] - 1) + elapsed) /
            self.stats["urls_analyzed"]
        )
    
    def _extract_context(self, event_data: Dict) -> Dict[str, Any]:
        """Extract analysis context from URL discovery event."""
        url = event_data["url"]
        response = event_data.get("response")

        context = self._build_empty_context(url)

        if response:
            self._populate_from_response(context, response)

        if not context["html_snippet"] and event_data.get("html"):
            context["html_snippet"] = event_data.get("html", "")[:5000]

        self._parse_url_params(context, url)
        context["tech_stack"] = self._detect_tech_stack(context)

        return context

    def _build_empty_context(self, url: str) -> Dict[str, Any]:
        """Build empty context structure."""
        return {
            "url": url,
            "method": "GET",
            "status_code": None,
            "headers": {},
            "html_snippet": "",
            "params": [],
            "tech_stack": [],
            "path": ""
        }

    def _populate_from_response(self, context: Dict, response: Any) -> None:
        """Populate context from HTTP response."""
        context["status_code"] = getattr(response, "status_code", None)
        context["headers"] = dict(getattr(response, "headers", {}))

        try:
            html_text = getattr(response, "text", "")
            context["html_snippet"] = html_text[:5000]
        except Exception:
            context["html_snippet"] = ""

    def _parse_url_params(self, context: Dict, url: str) -> None:
        """Parse URL to extract path and parameters."""
        try:
            from urllib.parse import urlparse, parse_qs
            parsed = urlparse(url)
            context["path"] = parsed.path

            if parsed.query:
                params = parse_qs(parsed.query)
                context["params"] = list(params.keys())
        except Exception as e:
            logger.warning(f"[{self.name}] Failed to parse URL: {e}")
    
    def _detect_tech_stack(self, context: Dict) -> List[str]:
        """Detect technology stack from headers and URL."""
        tech_stack = []
        
        url = context["url"].lower()
        server = context["headers"].get("Server", "").lower()
        x_powered_by = context["headers"].get("X-Powered-By", "").lower()
        
        # Web servers
        if "nginx" in server:
            tech_stack.append("Nginx")
        if "apache" in server:
            tech_stack.append("Apache")
        
        # Languages/Frameworks
        if ".php" in url or "php" in server or "php" in x_powered_by:
            tech_stack.append("PHP")
        if ".asp" in url or "asp.net" in x_powered_by:
            tech_stack.append("ASP.NET")
        if "express" in x_powered_by or "node" in server:
            tech_stack.append("Node.js/Express")
        
        # Databases (from errors in HTML)
        html = context.get("html_snippet", "").lower()
        if "mysql" in html:
            tech_stack.append("MySQL")
        if "postgresql" in html or "postgres" in html:
            tech_stack.append("PostgreSQL")
        if "oracle" in html:
            tech_stack.append("Oracle")
        if "mssql" in html or "sql server" in html:
            tech_stack.append("SQL Server")
        
        return tech_stack if tech_stack else ["Unknown"]
    
    async def _analyze_with_approach(
        self,
        context: Dict,
        approach: str
    ) -> Dict[str, Any]:
        """Analyze URL with single model using specific approach."""
        logger.info(f"[{self.name}] Analyzing with {approach} approach ({self.model})")

        try:
            response = await self._call_llm_for_approach(context, approach)

            if not response:
                raise Exception("Empty response from LLM")

            analysis = self._parse_analysis_response(response, approach)
            logger.info(f"[{self.name}] {approach} found {len(analysis['likely_vulnerabilities'])} potential vulns")

            return analysis

        except Exception as e:
            logger.error(f"[{self.name}] Analysis failed with {approach}: {e}", exc_info=True)
            return self._build_error_analysis(approach, e)

    async def _call_llm_for_approach(self, context: Dict, approach: str) -> str:
        """Call LLM with prompts for specific approach."""
        prompt = self._build_prompt(context, approach)
        system_prompt = self._get_system_prompt(approach)
        full_prompt = f"{system_prompt}\n\n{prompt}"

        return await llm_client.generate(
            prompt=full_prompt,
            module_name="AnalysisAgent",
            model_override=self.model,
            temperature=0.7,
            max_tokens=2000
        )

    def _parse_analysis_response(self, response: str, approach: str) -> Dict[str, Any]:
        """Parse XML response to extract vulnerabilities and framework."""
        parser = XmlParser()

        framework = parser.extract_tag(response, "framework") or "Unknown"
        vuln_contents = parser.extract_list(response, "vulnerability")

        likely_vulnerabilities = [
            self._parse_vulnerability(vc)
            for vc in vuln_contents
            if self._parse_vulnerability(vc) is not None
        ]

        return {
            "likely_vulnerabilities": likely_vulnerabilities,
            "framework_detected": framework,
            "model": self.model,
            "approach": approach,
            "timestamp": datetime.now().isoformat()
        }

    def _parse_vulnerability(self, vuln_content: str) -> Optional[Dict[str, Any]]:
        """Parse single vulnerability from XML content."""
        parser = XmlParser()

        v_type = parser.extract_tag(vuln_content, "type") or "Unknown"
        if v_type == "Unknown":
            return None

        v_conf_str = parser.extract_tag(vuln_content, "confidence") or "0.0"
        v_loc = parser.extract_tag(vuln_content, "location") or "unknown"
        v_reason = parser.extract_tag(vuln_content, "reasoning") or ""

        try:
            v_conf = float(v_conf_str)
        except Exception:
            v_conf = 0.5

        return {
            "type": v_type,
            "confidence": v_conf,
            "location": v_loc,
            "reasoning": v_reason
        }

    def _build_error_analysis(self, approach: str, error: Exception) -> Dict[str, Any]:
        """Build error analysis result."""
        return {
            "likely_vulnerabilities": [],
            "framework_detected": "Unknown",
            "model": self.model,
            "approach": approach,
            "error": str(error)
        }
    
    def _get_system_prompt(self, approach: str) -> str:
        """Get system prompt for specific analysis approach from external config."""
        personas = self.agent_config.get("personas", {})
        if approach in personas:
            return personas[approach].strip()
            
        # Fallback to general system prompt if persona not found
        return self.system_prompt or "You are an expert security analyst."
    

    def _build_prompt(self, context: Dict, persona: str) -> str:
        """Build analysis prompt for specific persona."""
        # Format parameters
        params_str = ", ".join(context["params"]) if context["params"] else "None"
        tech_str = " + ".join(context["tech_stack"])
        
        # Format headers (limit to important ones)
        important_headers = ["Server", "X-Powered-By", "Content-Type", "Set-Cookie"]
        headers_str = "\n".join([
            f"  {k}: {v}"
            for k, v in context["headers"].items()
            if k in important_headers
        ])
        
        prompt = f"""Analyze this web application URL for potential security vulnerabilities.

**URL**: {context['url']}
**Path**: {context['path']}
**HTTP Status**: {context['status_code']}
**Technology Stack**: {tech_str}
**Parameters**: {params_str}

**HTTP Headers**:
{headers_str if headers_str else "  (none)"}

**HTML Response** (first 5000 chars):
```html
{context['html_snippet'][:5000]}
```

**Task**: Identify potential vulnerabilities in this URL.

Focus on:
1. **SQL Injection**: Database error messages, suspicious parameters
2. **XSS (Cross-Site Scripting)**: Reflected input, unsafe output encoding
3. **Template Injection (CSTI/SSTI)**: Template engine usage patterns
4. **Path Traversal/LFI**: File inclusion patterns
5. **XXE**: XML parsing
6. **Command Injection**: Shell execution patterns

**Output Format** (XML-Like):
Return valid XML-like tags. Do NOT use markdown code blocks.

<analysis>
  <vulnerability>
    <type>SQLi</type>
    <confidence>0.9</confidence>
    <location>parameter 'id'</location>
    <reasoning>MySQL error message visible in response</reasoning>
  </vulnerability>
  <vulnerability>
    <type>XSS</type>
    <confidence>0.7</confidence>
    <location>parameter 'q'</location>
    <reasoning>Reflected input in search results</reasoning>
  </vulnerability>
  <framework>PHP + MySQL</framework>
  <notes>Additional observations</notes>
</analysis>

**Guidelines**:
- confidence: 0.0-1.0 (be realistic, not optimistic)
- Only include vulnerabilities you have evidence for
- Low confidence (< 0.5) for pure speculation
- High confidence (> 0.8) for clear indicators
- Use <vulnerability> tag for EACH finding.
- NO markdown formatting (```xml). Just raw tags."""
        
        return prompt
    
    def _consolidate_analyses(self, analyses: List[Dict], context: Dict) -> Dict[str, Any]:
        """Consolidate multiple model analyses into single report using consensus voting."""
        logger.info(f"[{self.name}] Consolidating {len(analyses)} analyses")

        vuln_votes = self._group_vulnerabilities_by_type(analyses)
        consensus_vulns, possible_vulns = self._calculate_consensus(vuln_votes)
        sorted_vulns = self._sort_by_priority(consensus_vulns + possible_vulns)

        report = self._build_final_report(
            context, analyses, consensus_vulns, possible_vulns, sorted_vulns
        )

        logger.info(f"[{self.name}] Consensus: {len(consensus_vulns)}, Possible: {len(possible_vulns)}")
        logger.info(f"[{self.name}] Attack priority: {report['attack_priority']}")

        return report

    def _group_vulnerabilities_by_type(self, analyses: List[Dict]) -> Dict[str, List[Dict]]:
        """Group vulnerabilities by type across all analyses."""
        vuln_votes = defaultdict(list)

        for analysis in analyses:
            for vuln in analysis.get("likely_vulnerabilities", []):
                vuln_type = vuln.get("type", "Unknown")
                vuln_votes[vuln_type].append({
                    **vuln,
                    "model": analysis.get("model"),
                    "persona": analysis.get("persona")
                })

        return vuln_votes

    def _calculate_consensus(self, vuln_votes: Dict) -> Tuple[List[Dict], List[Dict]]:
        """Calculate consensus and possible vulnerabilities from votes."""
        consensus_vulns = []
        possible_vulns = []

        for vuln_type, votes in vuln_votes.items():
            vuln_info = self._build_vuln_info(vuln_type, votes)

            if len(votes) >= self.consensus_votes:
                consensus_vulns.append(vuln_info)
                self.stats["consensus_count"] += 1
            else:
                possible_vulns.append(vuln_info)

        return consensus_vulns, possible_vulns

    def _build_vuln_info(self, vuln_type: str, votes: List[Dict]) -> Dict[str, Any]:
        """Build vulnerability info from votes."""
        avg_confidence = sum(v.get("confidence", 0) for v in votes) / len(votes)
        locations = list(set(v.get("location", "unknown") for v in votes))
        reasoning = [v.get("reasoning", "") for v in votes]
        models = [v.get("model", "") for v in votes]

        return {
            "type": vuln_type,
            "confidence": round(avg_confidence, 2),
            "votes": len(votes),
            "locations": locations,
            "reasoning": reasoning,
            "models": models
        }

    def _sort_by_priority(self, vulns: List[Dict]) -> List[Dict]:
        """Sort vulnerabilities by confidence Ã— severity weight."""
        SEVERITY_WEIGHTS = {
            "SQLi": 10, "RCE": 10, "XXE": 9, "SSTI": 8, "CSTI": 7,
            "LFI": 7, "SSRF": 7, "XSS": 6, "IDOR": 5, "CSRF": 4
        }

        return sorted(
            vulns,
            key=lambda v: v["confidence"] * SEVERITY_WEIGHTS.get(v["type"], 1),
            reverse=True
        )

    def _build_final_report(
        self, context: Dict, analyses: List[Dict],
        consensus_vulns: List[Dict], possible_vulns: List[Dict],
        sorted_vulns: List[Dict]
    ) -> Dict[str, Any]:
        """Build final consolidated report."""
        attack_priority = [
            v["type"] for v in sorted_vulns
            if v["confidence"] >= self.confidence_threshold
        ]

        skip_tests = [
            v["type"] for v in sorted_vulns
            if v["confidence"] < self.skip_threshold
        ]

        frameworks = [a.get("framework_detected", "Unknown") for a in analyses]
        framework = max(set(frameworks), key=frameworks.count) if frameworks else "Unknown"

        return {
            "url": context["url"],
            "framework_detected": framework,
            "tech_stack": context["tech_stack"],
            "consensus_vulns": consensus_vulns,
            "possible_vulns": possible_vulns,
            "attack_priority": attack_priority,
            "skip_tests": skip_tests,
            "total_models": len(analyses),
            "total_vulns_detected": len(consensus_vulns + possible_vulns),
            "timestamp": datetime.now().isoformat()
        }
    
    def _empty_report(self, url: str) -> Dict[str, Any]:
        """Generate empty report when analysis fails."""
        return {
            "url": url,
            "framework_detected": "Unknown",
            "tech_stack": [],
            "consensus_vulns": [],
            "possible_vulns": [],
            "attack_priority": [],
            "skip_tests": [],
            "total_models": 0,
            "total_vulns_detected": 0,
            "timestamp": datetime.now().isoformat(),
            "error": "Analysis failed"
        }
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get analysis statistics."""
        return {
            **self.stats,
            "cache_size": len(self.analysis_cache)
        }
    
    async def run_loop(self):
        """
        Event-driven run loop.
        AnalysisAgent is purely event-driven - it only responds to events.
        No polling needed.
        """
        logger.info(f"[{self.name}] Event-driven mode - listening for URL discoveries...")
        
        while self.running:
            await self.check_pause()
            # Just sleep - all work is event-driven
            await asyncio.sleep(5)
            
            # Log stats periodically
            if datetime.now().second % 30 == 0:
                stats = self.get_statistics()
                logger.debug(f"[{self.name}] Stats: {stats['urls_analyzed']} analyzed, {stats['cache_size']} cached")
    
    async def _save_report(self, url: str, report: Dict) -> None:
        """Save analysis report to disk for persistence and audit trail."""
        try:
            report_dir = self._create_report_directory(url)
            self._save_report_files(report_dir, url, report)
            logger.info(f"[{self.name}] ðŸ’¾ Report saved to {report_dir}")

        except Exception as e:
            logger.error(f"[{self.name}] Failed to save report: {e}", exc_info=True)

    def _create_report_directory(self, url: str) -> Path:
        """Create timestamped report directory."""
        from urllib.parse import urlparse

        parsed = urlparse(url)
        domain = (parsed.netloc or "unknown").replace(':', '_').replace('/', '_')

        timestamp = datetime.now()
        timestamp_str = timestamp.strftime('%Y%m%d_%H%M%S')
        milliseconds = timestamp.microsecond // 1000

        report_dirname = f"{domain}_{timestamp_str}_{milliseconds:03d}"
        report_dir = self.reports_dir / report_dirname
        report_dir.mkdir(parents=True, exist_ok=True)

        return report_dir

    def _save_report_files(self, report_dir: Path, url: str, report: Dict) -> None:
        """Save consolidated report and metadata to files."""
        # Save consolidated report
        report_file = report_dir / "consolidated_report.json"
        with open(report_file, "w") as f:
            json.dump(report, f, indent=2)

        # Save metadata
        metadata = self._build_metadata(url, report)
        metadata_file = report_dir / "metadata.json"
        with open(metadata_file, "w") as f:
            json.dump(metadata, f, indent=2)

    def _build_metadata(self, url: str, report: Dict) -> Dict[str, Any]:
        """Build metadata dictionary for report."""
        from urllib.parse import urlparse

        parsed = urlparse(url)
        domain = (parsed.netloc or "unknown").replace(':', '_').replace('/', '_')

        return {
            "url": url,
            "domain": domain,
            "timestamp": datetime.now().isoformat(),
            "approaches_count": len(self.approaches),
            "approaches_used": self.approaches,
            "model": self.model,
            "total_vulnerabilities": report.get("total_vulns_detected", 0),
            "consensus_count": len(report.get("consensus_vulns", [])),
            "attack_priority_count": len(report.get("attack_priority", []))
        }

