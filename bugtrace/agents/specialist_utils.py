"""
Specialist Agent Utilities

Shared utilities for specialist agents (XSSAgent, SQLiAgent, etc.)
to handle common operations like payload loading from JSON reports.

Version: 2.1.0
Date: 2026-02-02
"""

import json
from pathlib import Path
from typing import Dict, Any, Optional
from bugtrace.utils.logger import get_logger

logger = get_logger("agents.specialist_utils")


def load_full_payload_from_json(finding: Dict[str, Any]) -> Optional[str]:
    """
    Load full payload from JSON report when event payload is truncated.

    v2.1.0: Event payloads are truncated to 200 chars to keep events small.
    If a payload appears truncated, this function reads the full payload
    from the JSON report file generated by DASTySASTAgent.

    Args:
        finding: Finding dict from queue with optional _report_files field

    Returns:
        Full payload string if found, None if not available or error

    Example:
        >>> finding = {
        ...     "type": "XSS",
        ...     "parameter": "q",
        ...     "payload": "<script>alert(1)</script>...",  # Truncated to 200
        ...     "_report_files": {"json": "/path/to/1.json"}
        ... }
        >>> full_payload = load_full_payload_from_json(finding)
        >>> # Returns: "<script>alert(document.cookie)</script>{{long_payload}}"
    """
    # Get truncated payload from finding
    truncated_payload = finding.get("payload", "")

    # If payload is short enough, no need to read from JSON
    if len(truncated_payload) < 199:
        return truncated_payload

    # Check if report files reference exists
    report_files = finding.get("_report_files", {})
    json_path = report_files.get("json")

    if not json_path:
        logger.debug("[load_full_payload] No JSON report path available, using truncated payload")
        return truncated_payload

    # Read full payload from JSON
    try:
        json_file = Path(json_path)
        if not json_file.exists():
            logger.warning(f"[load_full_payload] JSON report not found: {json_path}")
            return truncated_payload

        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # Find matching vulnerability by type and parameter
        finding_type = finding.get("type", "").lower()
        finding_param = finding.get("parameter", "")

        for vuln in data.get("vulnerabilities", []):
            vuln_type = vuln.get("type", "").lower()
            vuln_param = vuln.get("parameter", "")

            # Match by type and parameter
            if finding_type in vuln_type and finding_param == vuln_param:
                full_payload = vuln.get("exploitation_strategy") or vuln.get("payload", "")
                if full_payload and len(full_payload) > len(truncated_payload):
                    logger.info(
                        f"[load_full_payload] Loaded full payload from JSON: "
                        f"{len(full_payload)} chars (was {len(truncated_payload)})"
                    )
                    return full_payload

        logger.debug(
            f"[load_full_payload] No matching vulnerability found in JSON for "
            f"{finding_type}/{finding_param}"
        )
        return truncated_payload

    except Exception as e:
        logger.warning(f"[load_full_payload] Failed to read JSON report: {e}")
        return truncated_payload


def load_full_finding_data(finding: Dict[str, Any]) -> Dict[str, Any]:
    """
    Load full finding data from JSON report, including reasoning and other fields
    that may be truncated in the event.

    Args:
        finding: Finding dict from queue with optional _report_files field

    Returns:
        Finding dict with full data merged from JSON, or original if not available

    Example:
        >>> finding = {"type": "SQLi", "reasoning": "Long reasoning..."[:500]}
        >>> full_finding = load_full_finding_data(finding)
        >>> # full_finding["reasoning"] now contains complete text
    """
    # Check if report files reference exists
    report_files = finding.get("_report_files", {})
    json_path = report_files.get("json")

    if not json_path:
        return finding

    try:
        json_file = Path(json_path)
        if not json_file.exists():
            return finding

        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # Find matching vulnerability
        finding_type = finding.get("type", "").lower()
        finding_param = finding.get("parameter", "")

        for vuln in data.get("vulnerabilities", []):
            vuln_type = vuln.get("type", "").lower()
            vuln_param = vuln.get("parameter", "")

            if finding_type in vuln_type and finding_param == vuln_param:
                # Merge full data into finding (prefer JSON values for truncated fields)
                merged = finding.copy()
                merged.update({
                    "payload": vuln.get("exploitation_strategy") or vuln.get("payload", merged.get("payload", "")),
                    "reasoning": vuln.get("reasoning", merged.get("reasoning", "")),
                    "fp_reason": vuln.get("fp_reason", merged.get("fp_reason", "")),
                    # Add any other fields that might be useful
                    "context": vuln.get("context", merged.get("context", "")),
                    "reflection_detected": vuln.get("reflection_detected", merged.get("reflection_detected", False)),
                })
                logger.info(f"[load_full_finding_data] Merged full data from JSON for {finding_type}/{finding_param}")
                return merged

        return finding

    except Exception as e:
        logger.warning(f"[load_full_finding_data] Failed to read JSON report: {e}")
        return finding


# =============================================================================
# VISUAL TELEMETRY INSTRUMENTATION (v4.2)
# =============================================================================
# Helper functions for specialist agents to report status to the dashboard.
# Usage: Call these functions at key points in start_queue_consumer() lifecycle.
# =============================================================================


def report_specialist_start(agent_name: str, queue_depth: int = 0):
    """
    Report that a specialist agent has started queue consumption.

    Call this at the start of start_queue_consumer().

    Args:
        agent_name: Agent name (e.g., 'SQLiAgent', 'XSSAgent')
        queue_depth: Initial queue depth
    """
    try:
        from bugtrace.core.ui import dashboard
        dashboard.update_specialist_status(
            agent_name,
            status="ACTIVE",
            queue=queue_depth,
            processed=0,
            vulns=0
        )
        logger.debug(f"[Telemetry] {agent_name} started (queue: {queue_depth})")
    except Exception as e:
        logger.debug(f"[Telemetry] Failed to report start: {e}")


def report_specialist_progress(agent_name: str, processed: int = None, queue: int = None):
    """
    Report specialist processing progress.

    Call this periodically during queue processing (e.g., after each item).

    Args:
        agent_name: Agent name
        processed: Number of items processed so far
        queue: Current queue depth (optional, for dynamic updates)
    """
    try:
        from bugtrace.core.ui import dashboard
        kwargs = {"status": "ACTIVE"}
        if processed is not None:
            kwargs["processed"] = processed
        if queue is not None:
            kwargs["queue"] = queue
        dashboard.update_specialist_status(agent_name, **kwargs)
    except Exception as e:
        logger.debug(f"[Telemetry] Failed to report progress: {e}")


def report_specialist_vuln(agent_name: str, vulns_count: int):
    """
    Report that a specialist found a vulnerability.

    Call this when a vulnerability is confirmed.

    Args:
        agent_name: Agent name
        vulns_count: Total vulnerabilities found by this agent
    """
    try:
        from bugtrace.core.ui import dashboard
        dashboard.update_specialist_status(agent_name, vulns=vulns_count)
        logger.debug(f"[Telemetry] {agent_name} reported {vulns_count} vulns")
    except Exception as e:
        logger.debug(f"[Telemetry] Failed to report vuln: {e}")


def report_specialist_done(agent_name: str, processed: int, vulns: int = 0):
    """
    Report that a specialist has finished queue consumption.

    Call this at the end of start_queue_consumer().

    Args:
        agent_name: Agent name
        processed: Total items processed
        vulns: Total vulnerabilities found
    """
    try:
        from bugtrace.core.ui import dashboard
        dashboard.update_specialist_status(
            agent_name,
            status="DONE",
            queue=0,
            processed=processed,
            vulns=vulns
        )
        logger.debug(f"[Telemetry] {agent_name} done (processed: {processed}, vulns: {vulns})")
    except Exception as e:
        logger.debug(f"[Telemetry] Failed to report done: {e}")


def report_specialist_wet_dry(agent_name: str, wet_count: int, dry_count: int):
    """
    Report WET→DRY transformation metrics for integrity verification.

    Call this after Phase A (deduplication) completes, before Phase B (exploitation).

    Args:
        agent_name: Agent name (e.g., 'XSSAgent', 'SQLiAgent')
        wet_count: Number of WET items consumed from queue
        dry_count: Number of DRY items after deduplication

    Example:
        >>> dry_list = await self.analyze_and_dedup_queue()
        >>> report_specialist_wet_dry(self.name, initial_depth, len(dry_list))
    """
    try:
        from bugtrace.core.batch_metrics import batch_metrics
        # Extract specialist key from agent name (e.g., 'XSSAgent' -> 'xss')
        specialist_key = agent_name.lower().replace("agent", "").strip()
        batch_metrics.record_specialist_wet_dry(specialist_key, wet_count, dry_count)
        logger.info(f"[Telemetry] {agent_name} WET→DRY: {wet_count} → {dry_count} ({wet_count - dry_count} dedup'd)")
    except Exception as e:
        logger.debug(f"[Telemetry] Failed to report WET/DRY: {e}")
