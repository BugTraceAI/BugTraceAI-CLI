import asyncio
from datetime import datetime
from bugtrace.agents.base import BaseAgent
from bugtrace.core.ui import dashboard
from bugtrace.memory.manager import memory_manager
from bugtrace.tools.exploitation.mutation import mutation_engine
from bugtrace.tools.external import external_tools
from bugtrace.core.config import settings
from bugtrace.core.llm_client import llm_client
from bugtrace.utils.logger import get_logger

# Import Conductor V2 for validation
from bugtrace.core.conductor import conductor

# Import Detectors (Check if they exist or mock if needed)
try:
    from bugtrace.tools.exploitation.xxe import xxe_detector
    from bugtrace.tools.exploitation.proto import proto_detector
    from bugtrace.tools.exploitation.csti import csti_detector
    from bugtrace.tools.exploitation.sqli import sqli_detector
    from bugtrace.tools.exploitation.header_injection import header_detector
except ImportError:
    # Safe fallback if some tools aren't fully modular yet
    pass

logger = get_logger("agents.exploit")

class ExploitAgent(BaseAgent):
    """
    Advanced Exploit Agent with 'Ladder Logic' (Light -> Heavy).
    Optimizes resources by running lightweight python-based checks first, 
    and only elevating to heavy tools (like SQLMap) when suspicious.
    
    EVENT BUS INTEGRATION (Phase 1 - COMPLETED):
    - Subscribes to: "new_input_discovered" (from ReconAgent)
    - Publishes: "vulnerability_detected" (to SkepticalAgent)
    """
    def __init__(self, event_bus=None):
        # Analysis-aware testing (Must be set BEFORE super().__init__ because _setup_event_subscriptions uses it)
        from bugtrace.core.config import settings
        self.use_analysis = getattr(settings, "ANALYSIS_ENABLE", True)
        self.confidence_threshold = getattr(settings, "ANALYSIS_CONFIDENCE_THRESHOLD", 0.7)
        self.analysis_reports = {}  # URL -> analysis report cache
        
        super().__init__("Exploit-1", "Offensive", event_bus=event_bus, agent_id="exploit_1")
        
        self.tested_vectors = set()
        self.mandatory_sqlmap = True  # Default mandatory SQLMap validation
        self.validated_findings = {"SQLi": set(), "XSS": set(), "Other": set()}  # Track validated params
    
    def _find_report_dir(self, url: str):
        """Find the most recent report directory for a given URL."""
        from pathlib import Path
        from urllib.parse import urlparse
        
        parsed = urlparse(url)
        # Fix for malformed URLs where query params might be appended without ?
        netloc = parsed.netloc
        if "&" in netloc and "?" not in url:
             netloc = netloc.split("&")[0]
             
        domain = netloc or "unknown"
        # CLEANUP: Remove port if present to match report dir format
        if ":" in domain:
            domain = domain.split(":")[0]
            
        domain = domain.replace('/', '_')
        
        reports_path = Path("reports")
        if not reports_path.exists():
            return None
        
        # Find all directories starting with this domain
        matching_dirs = [d for d in reports_path.iterdir() 
                        if d.is_dir() and d.name.startswith(domain + "_")]
        
        if not matching_dirs:
            return None
        
        # Return the most recent one (sorted by name which includes timestamp)
        return sorted(matching_dirs, reverse=True)[0]
    
    def _setup_event_subscriptions(self):
        """Subscribe to events from Event Bus."""
        if self.use_analysis:
            # Subscribe to analysis results (primary)
            self.event_bus.subscribe("url_analyzed", self.handle_url_analyzed)
            logger.info(f"[{self.name}] Subscribed to: url_analyzed (analysis-driven mode)")
        else:
            # Fallback to direct input discovery
            self.event_bus.subscribe("new_input_discovered", self.handle_new_input)
            logger.info(f"[{self.name}] Subscribed to: new_input_discovered (legacy mode)")
    
    def _cleanup_event_subscriptions(self):
        """Cleanup event subscriptions on agent stop."""
        if self.use_analysis:
            self.event_bus.unsubscribe("url_analyzed", self.handle_url_analyzed)
        else:
            self.event_bus.unsubscribe("new_input_discovered", self.handle_new_input)
        logger.info(f"[{self.name}] Unsubscribed from events")
    
    async def handle_new_input(self, data: dict):
        """
        EVENT HANDLER: Triggered when ReconAgent discovers new input.
        Executes IMMEDIATELY (~50ms) instead of polling (~10s).
        """
        # Phase 1: Extract and validate input
        input_info = self._extract_input_info(data)
        if not input_info:
            return

        url, input_label, input_type = input_info

        # Phase 2: Deduplication check
        if not self._check_and_mark_tested(url, input_label):
            return

        # Phase 3: Execute tests
        dashboard.update_task(self.name, status=f"Event: Testing {input_label}")
        await self._test_input(url, input_label, input_type)

    def _extract_input_info(self, data: dict) -> tuple:
        """Extract and log input information from event data."""
        url = data.get('url', '')
        input_details = data.get('input', {})
        input_label = input_details.get('name', 'unknown')
        input_type = input_details.get('type', '')

        logger.info(
            f"[{self.name}] ðŸ”¥ EVENT: new_input_discovered | "
            f"Input: {input_label} ({input_type}) at {url}"
        )

        return url, input_label, input_type

    def _check_and_mark_tested(self, url: str, input_label: str) -> bool:
        """Check if input was already tested and mark it as tested."""
        test_key = f"{url}:{input_label}"
        if test_key in self.tested_vectors:
            logger.debug(f"[{self.name}] Input already tested, skipping")
            return False

        self.tested_vectors.add(test_key)
        return True

    async def _test_input(self, url: str, input_label: str, input_type: str):
        """Execute test ladder for discovered input."""
        try:
            await self._check_waf(url)

            if settings.SAFE_MODE:
                logger.info(f"[{self.name}] SAFE MODE: Skipping {input_label}")
                return

            # Run ladder logic based on input type
            if ("search" in url.lower() or "q=" in url or input_type == "text"):
                await self._ladder_ui_attacks(url, input_label)

            if ("id=" in url or "cat=" in url or input_type == "number"):
                await self._ladder_sqli(url)

            await self._ladder_infrastructure(url)

            logger.info(f"[{self.name}] âœ… Completed testing {input_label}")

        except Exception as e:
            logger.error(f"[{self.name}] Handler error: {e}", exc_info=True)
    async def handle_url_analyzed(self, data: dict):
        """
        Handle url_analyzed event from AnalysisAgent.
        Reads analysis report and performs conditional exploitation.
        """
        report = data.get('report', {})
        url = report.get('url', '')
        
        if not url:
            logger.error(f"[{self.name}] No URL in analysis report")
            return
        
        logger.info(f"[{self.name}] ðŸ“Š Analysis report received for {url}")
        
        # Get attack priority and skip tests from report
        attack_priority = report.get('attack_priority', [])
        skip_tests = report.get('skip_tests', [])
        consensus_vulns = report.get('consensus_vulns', [])
        
        logger.info(f"[{self.name}] Attack priority: {attack_priority}")
        logger.info(f"[{self.name}] Consensus vulns: {len(consensus_vulns)}")
        logger.info(f"[{self.name}] Skip tests: {skip_tests}")
        
        # Check if any high-confidence XSS detected
        xss_vulns = [v for v in consensus_vulns if 'XSS' in v.get('type', '').upper()]
        
        if xss_vulns:
            logger.info(f"[{self.name}] ðŸŽ¯ {len(xss_vulns)} XSS vulnerabilities to validate")
            await self._validate_xss_from_report(url, xss_vulns)
        
        # Check for SQLi
        sqli_vulns = [v for v in consensus_vulns if 'SQL' in v.get('type', '').upper()]
        
        if sqli_vulns and 'SQLi' in attack_priority:
            logger.info(f"[{self.name}] ðŸŽ¯ {len(sqli_vulns)} SQLi vulnerabilities to test")
            await self._ladder_sqli(url)
        
        logger.info(f"[{self.name}] âœ… Completed conditional exploitation for {url}")
    
    async def _validate_xss_from_report(self, url: str, xss_vulns: list):
        """
        Validate XSS vulnerabilities using browser + vision model.
        Cost-conscious: Only validates with screenshots.

        IMPROVED: Uses actual parameter names from analysis locations.
        """
        vision_calls = 0
        max_calls = getattr(settings, 'VALIDATION_MAX_VISION_CALLS_PER_URL', 3)

        for vuln in xss_vulns:
            if vision_calls >= max_calls:
                logger.warning(f"[{self.name}] Max vision calls reached ({max_calls})")
                break

            # Phase 1: Extract test parameters
            test_params = self._extract_xss_test_params(vuln)

            # Phase 2: Test each parameter with payloads
            confirmed, vision_calls = await self._test_xss_params(
                url, test_params, vuln, vision_calls, max_calls
            )

            if confirmed:
                return  # Stop testing on first confirmation

            logger.info(f"[{self.name}] âŒ No XSS confirmed for this vulnerability")

    def _extract_xss_test_params(self, vuln: dict) -> list[str]:
        """Extract parameter names from vulnerability locations."""
        fallback_params = ['searchFor', 'q', 'query', 'search', 'keyword', 'name', 'id', 'test']
        locations = vuln.get('locations', [])

        logger.info(f"[{self.name}] Testing {vuln.get('type', 'XSS')} (confidence: {vuln.get('confidence', 0.0)})")
        logger.info(f"[{self.name}] Locations: {locations}")

        param_names = []
        for loc in locations:
            loc_lower = str(loc).lower()
            if 'searchfor' in loc_lower:
                param_names.append('searchFor')
            if 'parameter' in loc_lower:
                import re
                match = re.search(r"parameter['\s]+(\w+)", loc_lower)
                if match:
                    param_names.append(match.group(1))

        return param_names if param_names else fallback_params[:3]

    async def _test_xss_params(
        self,
        url: str,
        test_params: list[str],
        vuln: dict,
        vision_calls: int,
        max_calls: int
    ) -> tuple[bool, int]:
        """Test XSS payloads across parameters."""
        from pathlib import Path
        from urllib.parse import urlparse

        payloads = [
            "<script>alert('XSS-TEST-BUGTRACE')</script>",
            "<img src=x onerror=alert('XSS-TEST')>",
            "'\"><script>alert('XSS')</script>"
        ]

        parsed = urlparse(url)
        base_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        confidence = vuln.get('confidence', 0.0)

        for param in test_params:
            for payload in payloads[:1]:  # Only test first payload per param (cost saving)
                if vision_calls >= max_calls:
                    break

                test_url = f"{base_url}?{param}={payload}"
                logger.info(f"[{self.name}] Testing XSS: {param}={payload[:30]}...")

                try:
                    screenshot_path = await self._test_xss_with_browser(test_url, payload)

                    if screenshot_path and Path(screenshot_path).exists():
                        is_confirmed = await self._validate_xss_screenshot(screenshot_path)
                        vision_calls += 1

                        if is_confirmed:
                            logger.info(f"[{self.name}] âœ… XSS CONFIRMED on {param}!")
                            await self._report_finding(url, "XSS", confidence, screenshot_path, validated=True)
                            return True, vision_calls
                        else:
                            logger.info(f"[{self.name}] âŒ XSS not confirmed on {param}")
                    else:
                        logger.warning(f"[{self.name}] No screenshot captured for XSS test")

                except Exception as e:
                    logger.error(f"[{self.name}] XSS validation failed: {e}", exc_info=True)

        return False, vision_calls

    
    async def _test_xss_with_browser(self, url: str, payload: str) -> str:
        """
        Test XSS using browser and capture screenshot.
        Returns path to screenshot if alert is detected.
        """
        try:
            # Phase 1: Setup browser test environment
            screenshots_dir = self._browser_setup_screenshots_dir(url)
            if not screenshots_dir:
                return None

            # Phase 2: Execute browser verification
            temp_screenshot, logs, triggered = await self._browser_execute_verification(url)
            if not temp_screenshot:
                return None

            # Phase 3: Move screenshot to report directory
            final_path = self._browser_move_screenshot(temp_screenshot, screenshots_dir)

            # Phase 4: Log result
            self._browser_log_result(triggered, logs, final_path)
            return str(final_path)

        except Exception as e:
            logger.error(f"[{self.name}] Browser test failed: {e}", exc_info=True)
            return None

    def _browser_setup_screenshots_dir(self, url: str):
        """Setup screenshots directory for browser test."""
        from pathlib import Path

        report_dir = self._find_report_dir(url)
        if not report_dir:
            logger.error(f"[{self.name}] No report directory found for {url}")
            return None

        screenshots_dir = report_dir / "screenshots"
        screenshots_dir.mkdir(parents=True, exist_ok=True)
        return screenshots_dir

    async def _browser_execute_verification(self, url: str):
        """Execute browser XSS verification."""
        from bugtrace.tools.visual.browser import BrowserManager

        browser = BrowserManager()
        temp_screenshot, logs, triggered = await browser.verify_xss(url)

        if not temp_screenshot:
            logger.warning(f"[{self.name}] No screenshot captured from verify_xss")

        return temp_screenshot, logs, triggered

    def _browser_move_screenshot(self, temp_screenshot: str, screenshots_dir) -> str:
        """Move screenshot to final report directory."""
        from pathlib import Path
        import shutil

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        final_path = screenshots_dir / f"xss_test_{timestamp}.png"

        try:
            shutil.move(temp_screenshot, final_path)
            return final_path
        except Exception as e:
            logger.error(f"[{self.name}] Failed to move screenshot: {e}", exc_info=True)
            return temp_screenshot

    def _browser_log_result(self, triggered: bool, logs: list, final_path: str):
        """Log browser test result."""
        if triggered:
            logger.info(f"[{self.name}] ðŸŽ‰ Alert dialog detected via BrowserManager")
        else:
            logger.info(f"[{self.name}] No alert dialog detected (Logs: {len(logs)})")
    
    async def _validate_screenshot(self, screenshot_path: str, vuln_type: str = "XSS") -> bool:
        """
        Validate vulnerability using vision model (cost-conscious).

        CRITICAL: Uses vuln_type to ask the CORRECT question.
        - XSS: Looks for JavaScript alert() dialog box
        - SQLi: Looks for SQL error messages in the page
        - Generic: General vulnerability indicators

        Returns True if the expected vulnerability indicator is visible.
        """
        try:
            if not self._check_vision_enabled():
                return False

            # Build and execute validation prompt
            prompt = self._build_vision_prompt(vuln_type)
            logger.info(f"[{self.name}] Vision validation for {vuln_type}: {prompt[:60]}...")

            result = await llm_client.generate_with_image(
                prompt=prompt,
                image_path=screenshot_path,
                model_override=getattr(settings, 'VALIDATION_VISION_MODEL', 'qwen/qwen3-vl-8b-thinking'),
                module_name=f"ExploitAgent-Vision-{vuln_type}"
            )

            # Parse response - strict YES matching
            is_confirmed = result.strip().upper().startswith("YES")

            logger.info(f"[{self.name}] Vision validation ({vuln_type}): {is_confirmed}")
            logger.debug(f"[{self.name}] Vision response: {result}")

            return is_confirmed

        except Exception as e:
            logger.error(f"[{self.name}] Vision validation error: {e}", exc_info=True)
            return False

    def _check_vision_enabled(self) -> bool:
        """Check if vision validation is enabled."""
        vision_enabled = getattr(settings, 'VALIDATION_VISION_ENABLED', True)
        if not vision_enabled:
            logger.info(f"[{self.name}] Vision validation disabled")
        return vision_enabled

    def _build_vision_prompt(self, vuln_type: str) -> str:
        """Build vision validation prompt for specific vulnerability type."""
        prompts = {
            "XSS": (
                "This is a browser screenshot. Look carefully at the page content. "
                "Is there ANY of the following visible on this page? "
                "1) Text containing 'BUGTRACE-XSS-CONFIRMED' or 'BUGTRACE-XSS' "
                "2) A JavaScript alert popup/dialog box "
                "3) Red colored text showing a domain name (like 'example.com') "
                "4) Any text that looks like it was injected (unusual formatting, yellow background). "
                "Answer ONLY: YES or NO"
            ),
            "SQLi": (
                "This is a browser screenshot. Does this page show a SQL database error message? "
                "Look for text like 'SQL syntax error', 'mysql', 'ORA-', 'PostgreSQL', 'SQLite', 'ODBC', "
                "or stack traces mentioning database. Answer ONLY: YES or NO"
            ),
            "LFI": (
                "This is a browser screenshot. Does this page show content from a local file "
                "like /etc/passwd (showing root:x:0:0) or Windows system files? Answer ONLY: YES or NO"
            ),
            "GENERIC": (
                "This is a browser screenshot. Does this page show any error message, "
                "stack trace, or sensitive information disclosure? Answer ONLY: YES or NO"
            )
        }

        # Use external system_prompt if available as override/template
        prompt = prompts.get(vuln_type.upper(), prompts["GENERIC"])
        if self.system_prompt:
            prompt = self.system_prompt.replace("{vuln_type}", vuln_type)

        return prompt
    
    # Backward compatibility alias
    async def _validate_xss_screenshot(self, screenshot_path: str) -> bool:
        """Legacy XSS-specific validation. Calls generic validator with XSS type."""
        return await self._validate_screenshot(screenshot_path, "XSS")
    
    async def _report_finding(self, url: str, vuln_type: str, confidence: float, screenshot_path: str, validated: bool):
        """
        Report validated finding with proper structure.
        """
        from pathlib import Path
        import json
        
        # Find report directory for this URL
        report_dir = self._find_report_dir(url)
        if not report_dir:
            logger.error(f"[{self.name}] No report directory found for {url}")
            return
        
        finding = {
            "url": url,
            "type": vuln_type,
            "confidence": confidence,
            "screenshot": screenshot_path,
            "validated": validated,
            "timestamp": datetime.now().isoformat(),
            "validation_method": "vision_model" if validated else "browser_only"
        }
        
        # Load existing validated findings or create new
        findings_file = report_dir / "validated_findings.json"
        if findings_file.exists():
            with open(findings_file) as f:
                findings = json.load(f)
        else:
            findings = {"findings": [], "summary": {"total": 0, "validated": 0}}
        
        # Add new finding
        findings["findings"].append(finding)
        findings["summary"]["total"] = len(findings["findings"])
        findings["summary"]["validated"] = sum(1 for f in findings["findings"] if f["validated"])
        
        # Save
        with open(findings_file, "w") as f:
            json.dump(findings, f, indent=2)
        
        # Emit event
        await self.event_bus.emit("vulnerability_confirmed", finding)
        
        logger.info(f"[{self.name}] ðŸ“ Finding saved: {vuln_type} at {url}")
        logger.info(f"[{self.name}] ðŸ“¸ Screenshot: {screenshot_path}")
        logger.info(f"[{self.name}] âœ… Validated: {validated}")

    async def run_loop(self):
        """
        DUAL MODE RUN LOOP (Polling + Events for safety).

        TODO: After confirming events work, remove polling block.
        """
        dashboard.log(f"[{self.name}] Agent Online. Dual Mode (Polling + Events).", "INFO")
        logger.info(f"[{self.name}] Started - Listening for events...")

        while self.running:
            await self.check_pause()

            # Phase 1: Get pending targets
            pending_targets = self._get_pending_targets()

            if not pending_targets:
                self._idle_status_update()
                await asyncio.sleep(2)
                continue

            dashboard.log(f"[{self.name}] Engaged {len(pending_targets)} targets (polling).", "INFO")

            # Phase 2: Process each target
            await self._process_targets(pending_targets)

            await asyncio.sleep(5)

    def _get_pending_targets(self) -> list:
        """Get untested targets from attack surface."""
        attack_surface = memory_manager.get_attack_surface()

        pending_targets = []
        for n in attack_surface:
            has_url = n.get('url') or n.get('properties', {}).get('url')
            if n.get('label') not in self.tested_vectors and has_url:
                pending_targets.append(n)

        return pending_targets

    def _idle_status_update(self):
        """Update dashboard when idle."""
        if datetime.now().second % 10 == 0:
            dashboard.update_task(self.name, status="Listening...")

    async def _process_targets(self, pending_targets: list):
        """Process list of pending targets."""
        for target in pending_targets:
            await self.check_pause()
            if not self.running:
                break

            # Extract target info
            label = target.get('label', 'Unknown')
            url = target.get('url') or target.get('properties', {}).get('url')
            if not url:
                continue

            node_type = target.get('type', 'Unknown')
            self.tested_vectors.add(label)
            dashboard.update_task(self.name, status=f"Polling: {label}")

            # Execute attack ladder
            await self._execute_attack_ladder(url, label, node_type)

            await asyncio.sleep(1)

    async def _execute_attack_ladder(self, url: str, label: str, node_type: str):
        """Execute attack ladder for a target."""
        try:
            await self._check_waf(url)

            if settings.SAFE_MODE:
                dashboard.log(f"[{self.name}] SAFE MODE: Skipping {label}", "WARN")
                return

            # Skip XSS in polling when using analysis-driven mode
            if not self.use_analysis:
                if "xss" in label.lower() or "search" in url or node_type == "Input":
                    await self._ladder_ui_attacks(url, label)

            if "sqli" in label.lower() or "id=" in url or node_type == "Input":
                await self._ladder_sqli(url)

            await self._ladder_infrastructure(url)

        except Exception as e:
            logger.error(f"Error exploiting {url}: {e}", exc_info=True)

    async def _check_waf(self, url: str):
        """Checks for WAF presence."""
        pass


    async def _run_light_sqli_check(self, url: str) -> Optional[Tuple[str, Optional[str]]]:
        """Run lightweight SQLi detection."""
        try:
            return await sqli_detector.check(url)
        except Exception as e:
            logger.warning(f"SQLi Light Check failed: {e}")
            return None

    def _build_sqli_finding(self, url: str, msg: str, screenshot: Optional[str]) -> Dict:
        """Build SQLi finding data structure."""
        return {
            "finding_id": f"sqli_{url}",
            "type": "SQLi",
            "url": url,
            "payload": msg,
            "confidence": 0.9,
            "evidence": {
                "error_message": msg,
                "screenshot": screenshot,
                "response": {
                    "status_code": 500,
                    "body": msg,
                    "headers": {}
                }
            },
            "description": f"SQL Injection vulnerability detected via error-based technique. The server returned a database error message indicating injectable parameter. Error: {msg[:200]}",
            "reproduction": f"# SQLi detected at: {url}\n# Use sqlmap for full exploitation:\nsqlmap -u \"{url}\" --batch --dbs",
            "detected_by": self.name,
            "timestamp": datetime.now().isoformat()
        }

    async def _validate_and_emit_sqli(self, finding_data: Dict, url: str, msg: str) -> bool:
        """Validate SQLi finding with Conductor and emit event."""
        # VALIDATION: Check with Conductor V2
        is_valid, reason = conductor.validate_finding(finding_data)

        if not is_valid:
            logger.warning(f"[{self.name}] SQLi finding BLOCKED by validation: {reason}")
            dashboard.log(f"[{self.name}] SQLi blocked: {reason}", "WARN")
            return False

        # Validation passed - emit event
        dashboard.add_finding("SQL Injection", msg, "CRITICAL")
        logger.info(f"[{self.name}] SQLi CONFIRMED (validated): {msg}")

        memory_manager.add_node("FindingCandidate", finding_data["finding_id"], {
            "url": url, "payload": msg, "status": "FIRED", "type": "SQLi"
        })

        # Mark param as validated
        self._mark_param_validated(url, "SQLi")

        # EVENT: Emit vulnerability_detected
        await self.event_bus.emit("vulnerability_detected", finding_data)
        logger.info(f"[{self.name}] ðŸ“¢ EVENT EMITTED: vulnerability_detected (SQLi - validated)")

        return True

    def _mark_param_validated(self, url: str, vuln_type: str):
        """Mark a parameter as validated for a vulnerability type."""
        from urllib.parse import urlparse, parse_qs
        parsed = urlparse(url)
        params = list(parse_qs(parsed.query).keys()) if parsed.query else ["path"]
        param_key = f"{parsed.netloc}:{params[0] if params else 'unknown'}"

        self.validated_findings[vuln_type].add(param_key)
        logger.critical(f"[{self.name}] ðŸŽ¯ {vuln_type} VALIDATED on {param_key} - stopping {vuln_type} tests on this param")

    async def _run_sqlmap_validation(self, url: str) -> bool:
        """Run SQLMap for mandatory validation."""
        if "?" not in url or "=" not in url:
            return False

        if not self.mandatory_sqlmap:
            logger.warning(f"[{self.name}] âš ï¸ Skipping SQLMap (MANDATORY_SQLMAP_VALIDATION=False)")
            return False

        dashboard.update_task(self.name, status=f"SQLi Heavy: SQLMap on {url}")

        from bugtrace.tools.visual.browser import browser_manager
        session_data = await browser_manager.get_session_data()
        cookies = session_data.get("cookies", [])

        logger.info(f"[{self.name}] ðŸ” MANDATORY SQLMap validation: {url}")
        return await external_tools.run_sqlmap(url, cookies=cookies)

    async def _ladder_sqli(self, url: str):
        """
        SQLi Ladder:
        1. Light Check (Python Detector)
        2. Heavy Check (SQLMap) - Only if suspicious.
        """
        dashboard.update_task(self.name, status=f"SQLi Ladder: {url}")

        # Step 1: Light Check
        if await self._sqli_light_check_phase(url):
            return  # Confirmed via light check, stop testing

        # Step 2: Heavy Check (SQLMap)
        await self._sqli_sqlmap_phase(url)

    async def _sqli_light_check_phase(self, url: str) -> bool:
        """Execute SQLi light check phase (Python detector)."""
        res_tuple = await self._run_light_sqli_check(url)
        if not res_tuple:
            return False

        msg, screenshot = res_tuple if isinstance(res_tuple, tuple) else (res_tuple, None)
        finding_data = self._build_sqli_finding(url, msg, screenshot)

        return await self._validate_and_emit_sqli(finding_data, url, msg)

    async def _sqli_sqlmap_phase(self, url: str):
        """Execute SQLi SQLMap validation phase."""
        is_vuln = await self._run_sqlmap_validation(url)
        if not is_vuln:
            if "?" in url and "=" in url and self.mandatory_sqlmap:
                logger.info(f"[{self.name}] âŒ SQLMap rejected - false positive discarded")
            return

        # SQLMap CONFIRMED - create and emit finding
        self._mark_param_validated(url, "SQLi")
        finding_data = self._build_sqlmap_finding(url)

        if await self._validate_and_emit_sqli(finding_data, url, "SQLMap confirmed"):
            await self.event_bus.emit("vulnerability_detected", finding_data)
            logger.info(f"[{self.name}] ðŸ“¢ EVENT EMITTED: vulnerability_detected (SQLMap - validated)")

    def _build_sqlmap_finding(self, url: str) -> Dict:
        """Build SQLMap confirmed finding data structure."""
        return {
            "finding_id": f"sqli_map_{url}",
            "type": "SQLi",
            "url": url,
            "payload": "SQLMap Confirmed",
            "confidence": 1.0,
            "evidence": {
                "error_message": "SQLMap confirmed vulnerability",
                "response": {
                    "status_code": 200,
                    "body": "SQLMap output",
                    "headers": {}
                }
            },
            "description": f"SQL Injection vulnerability confirmed by SQLMap automated scanner. Full database enumeration and data extraction is possible.",
            "reproduction": f"sqlmap -u \"{url}\" --batch --dbs --tables",
            "detected_by": self.name,
            "timestamp": datetime.now().isoformat()
        }

    async def _ladder_ui_attacks(self, url: str, label: str):
        """UI Attacks Ladder (XSS, CSTI) with Conductor V2 validation."""
        dashboard.update_task(self.name, status=f"XSS Ladder: {url}")

        # Phase 1: XSS Testing
        await self._ui_attacks_xss_phase(url, label)

        # Phase 2: CSTI Testing
        await self._ui_attacks_csti_phase(url)

    async def _ui_attacks_xss_phase(self, url: str, label: str):
        """Execute XSS testing phase with mutation and validation."""
        # Generate and validate payload
        base_payload = "<script>alert(document.domain)</script>"
        mutated = await mutation_engine.mutate_payload(base_payload, f"Target: {url}")

        if not conductor.validate_payload(mutated, "XSS"):
            logger.warning(f"[{self.name}] XSS payload INVALID: {mutated[:50]}")
            return

        # Build and validate finding
        finding_data = self._build_xss_finding(url, label, mutated)
        is_valid, reason = conductor.validate_finding(finding_data)

        if not is_valid:
            logger.warning(f"[{self.name}] XSS finding BLOCKED: {reason}")
            dashboard.log(f"[{self.name}] XSS blocked: {reason}", "WARN")
            return

        # Store and emit validated finding
        memory_manager.add_node("FindingCandidate", f"xss_{label}", {
            "url": url, "payload": mutated, "status": "TO_VERIFY", "type": "XSS"
        })

        await self.event_bus.emit("vulnerability_detected", finding_data)
        logger.info(f"[{self.name}] ðŸ“¢ EVENT EMITTED: vulnerability_detected (XSS - validated)")

    def _build_xss_finding(self, url: str, label: str, payload: str) -> Dict:
        """Build XSS finding data structure."""
        return {
            "finding_id": f"xss_{label}",
            "type": "XSS",
            "url": url,
            "payload": payload,
            "confidence": 0.7,
            "evidence": {
                "alert_triggered": True,
                "screenshot": None,
                "response": {
                    "status_code": 200,
                    "body": "",
                    "headers": {}
                }
            },
            "description": f"Cross-Site Scripting (XSS) vulnerability detected. Payload was reflected or executed in the page context. Payload: {payload[:100]}",
            "reproduction": f"# Open in browser with payload:\n{url}",
            "detected_by": self.name,
            "timestamp": datetime.now().isoformat()
        }

    async def _ui_attacks_csti_phase(self, url: str):
        """Execute CSTI testing phase."""
        res = await csti_detector.check(url)
        if res:
            dashboard.add_finding("CSTI", res, "HIGH")

    async def _ladder_infrastructure(self, url: str):
        """Infrastructure Ladder (Headers, Proto, XXE)."""
        # Header Injection
        res = await header_detector.check(url)
        if res:
            msg = res[0] if isinstance(res, tuple) else res
            dashboard.add_finding("Header Injection", msg, "MEDIUM")

        # Prototype Pollution
        res = await proto_detector.check(url)
        if res:
             dashboard.add_finding("Prototype Pollution", res, "HIGH")
