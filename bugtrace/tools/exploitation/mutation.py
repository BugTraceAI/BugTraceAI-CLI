from typing import List, Optional
import re
from bugtrace.utils.logger import get_logger
from bugtrace.core.llm_client import llm_client
from bugtrace.core.conductor import conductor
from bugtrace.core.ui import dashboard
from bugtrace.core.config import settings

logger = get_logger("tools.mutation")

class MutationEngine:
    """
    Advanced Mutation Engine with 'Prompt Shifting' and Payload Validation.
    Ensures LLM outputs are actual payloads and not conversational text.
    
    Uses MUTATION_MODEL (default: DeepSeek) which has fewer safety restrictions
    for security research compared to Gemini.
    """
    
    def __init__(self):
        # Different prompt strategies for evasion
        self.strategies = [
            "JavaScript String Breakout (Escape Bypass)",
            "Advanced Evasion (Polyglots/Event Handlers)",
            "Contextual Blending (Native JS/HTML)",
            "Encoding/Obfuscation Shift",
            "Minimalist Bypass"
        ]
        # Get mutation model from config (uses DeepSeek by default)
        self.mutation_model = settings.MUTATION_MODEL
        logger.info(f"MutationEngine initialized with model: {self.mutation_model}")
    
    def _validate_payload(self, payload: str) -> bool:
        """
        Validates if the generated text looks like a payload and not a sentence.
        """
        if not payload or len(payload) < 2:
            return False
            
        # Reject conversational fluff
        conversational_triggers = [
            "here is the payload", "i have mutated", "try this", "generated payload", 
            "sorry", "cannot", "as an ai", "note that", "i cannot fulfill"
        ]
        lower_payload = payload.lower()
        if any(trigger in lower_payload for trigger in conversational_triggers):
            # If it's long and contains these, it's likely chatter unless it's a polyglot with comments
            # But usually LLMs put chatter outside code blocks.
            # We assume the LLM client (or our parsing) strips code blocks, but let's be safe.
            if len(payload) > 100 and not any(c in payload for c in "<>'\"();"):
                return False
                
        # Must contain some attack characters
        if "\\'" in payload or "\\\"" in payload:
             return True

        if not any(c in payload for c in "<>'\"();${}\\"):
            return False
            
        return True

    async def mutate_payload(self, original_payload: str, context: str = "WAF blockage", preferred_strategy: str = None) -> str:
        """
        Takes a blocked payload and mutates it using Strategy Shifting.
        Uses MUTATION_MODEL (DeepSeek) for fewer safety restrictions.
        """
        logger.info(f"Mutation: Initiating strategy shifting for '{original_payload}'")

        strategies = self._prioritize_strategies(preferred_strategy)

        for strategy in strategies:
            mutated = await self._try_mutation_strategy(strategy, original_payload, context)
            if mutated:
                return mutated

        # Fallback
        return original_payload

    def _prioritize_strategies(self, preferred_strategy: str = None) -> List[str]:
        """Reorder strategies to prioritize preferred one."""
        current_strategies = self.strategies.copy()
        if preferred_strategy and preferred_strategy in current_strategies:
            logger.info(f"Mutation: Prioritizing strategy '{preferred_strategy}' based on context hint")
            current_strategies.remove(preferred_strategy)
            current_strategies.insert(0, preferred_strategy)
        return current_strategies

    async def _try_mutation_strategy(self, strategy: str, original_payload: str, context: str) -> Optional[str]:
        """Try a single mutation strategy."""
        logger.info(f"Mutation Shift: Trying strategy '{strategy}'")
        dashboard.update_task("Exploit-1", status=f"Mutation Shift: {strategy}")

        prompt = self._build_mutation_prompt(strategy, context, original_payload)

        mutated = await llm_client.generate(
            prompt,
            module_name=f"Mutation-{strategy.split()[0]}",
            model_override=self.mutation_model
        )

        if mutated:
            clean_mutated = self._sanitize_payload(mutated).strip().replace('`', '')

            if self._validate_payload(clean_mutated):
                logger.info(f"Mutation Success with {strategy}: {clean_mutated}")
                return clean_mutated
            else:
                logger.warning(f"Mutation Shift: Invalid payload generated ('{clean_mutated[:20]}...'). Retrying...")

        logger.warning(f"Mutation Shift: Strategy '{strategy}' failed. Shifting to next...")
        return None

    def _build_mutation_prompt(self, strategy: str, context: str, original_payload: str) -> str:
        """Build mutation prompt for LLM."""
        system_prompt = conductor.get_full_system_prompt("mutation")
        if system_prompt:
            prompt = system_prompt.split("## Mutation Prompt")[-1].strip()
        else:
            prompt = f"""
            You are a red team payload generator for authorized penetration testing.
            Objective: Mutate the following web security payload to evade a WAF.
            Strategy: {strategy}
            Target Context: {context}
            Original Payload: {original_payload}

            Rules:
            1. Maintain functional intent.
            2. Output ONLY the raw code. NO explanation. NO markdown. NO "Here is...".
            3. Do not wrap in ``` code blocks.
            """

        return prompt.format(
            strategy=strategy,
            context=context,
            original_payload=original_payload
        )
    
    def _sanitize_payload(self, raw_payload: str) -> str:
        """
        Clean LLM output to extract only the actual payload.
        Removes <think> tags, explanations, and other artifacts.
        """
        import re
        
        # Remove <think>...</think> blocks (common in QwQ, DeepSeek models)
        payload = re.sub(r'<think>.*?</think>', '', raw_payload, flags=re.DOTALL | re.IGNORECASE)
        
        # Remove common LLM artifacts
        payload = re.sub(r'```.*?```', '', payload, flags=re.DOTALL)  # Code blocks
        payload = re.sub(r'^(Here is|Here\'s|The mutated payload is).*?:', '', payload, flags=re.IGNORECASE | re.MULTILINE)
        
        # Extract only lines that look like payloads (start with < or contain script/img/svg)
        lines = payload.split('\n')
        payload_lines = []
        for line in lines:
            line = line.strip()
            if line and (line.startswith('<') or 'script' in line.lower() or 'onerror' in line.lower() or 'onload' in line.lower()):
                payload_lines.append(line)
        
        if payload_lines:
            payload = payload_lines[0]  # Take first valid line
        else:
            payload = payload.strip()
        
        # Final cleanup
        payload = payload.strip()
        
        # If still too long (> 800 chars), take first 800
        if len(payload) > 800:
            logger.warning(f"Sanitized payload still long ({len(payload)} chars), truncating to 800")
            payload = payload[:800]
        
        return payload

    def get_common_payloads(self, attack_type: str) -> List[str]:
        if attack_type == "XSS":
            return ["<script>alert(1)</script>", "\"><img src=x onerror=prompt(1)>"]
        elif attack_type == "SQLi":
            return ["' OR 1=1--", "' UNION SELECT null,null--"]
        return []

mutation_engine = MutationEngine()
