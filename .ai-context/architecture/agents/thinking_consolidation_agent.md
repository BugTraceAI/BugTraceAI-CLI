# ThinkingConsolidationAgent - El Cerebro del Pipeline

> **Fase**: 3 (Strategy)  
> **Rol**: Coordinador Central, Deduplicador, Clasificador y Priorizador  
> **Clase**: `bugtrace.agents.thinking_consolidation_agent.ThinkingConsolidationAgent`  
> **Archivo**: `bugtrace/agents/thinking_consolidation_agent.py`

---

## Overview

**ThinkingConsolidationAgent** es el **cerebro central** del pipeline de BugTraceAI, posicionado entre la Fase 2 (Discovery) y la Fase 4 (Exploitation). Es el agente m√°s cr√≠tico del sistema porque **decide qu√© findings pasan a los specialist agents** y cu√°les son descartados.

Su misi√≥n: **Convertir el caos de Discovery (miles de findings) en un plan de batalla ordenado y optimizado**.

### üéØ **Responsabilidades Principales**

| Responsabilidad | Descripci√≥n | Impacto |
|-----------------|-------------|---------|
| **Deduplicaci√≥n Masiva** | Agrupa 1000 URLs con `?id=` ‚Üí 1 tarea √∫nica | Reduce 90% de trabajo redundante |
| **Clasificaci√≥n Sem√°ntica** | `?q=` ‚Üí XSS, `?file=` ‚Üí LFI, `?id=` ‚Üí SQLi | Routing inteligente a specialists |
| **FP Filtering** | fp_confidence < 0.5 ‚Üí FILTERED (excepto SQLi) | Ahorra tiempo de specialists |
| **Priorizaci√≥n** | Formula weighted score: severity + confidence + skeptical | Ataca lo relevante primero |
| **Batch Processing** | Acumula findings y procesa en batches | Reduce overhead de LLM calls |
| **Queue Distribution** | Enruta a colas de specialists (`xss`, `sqli`, etc.) | Orquesta enjambre de agentes |

---

## Arquitectura del Agente

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ       THINKING CONSOLIDATION AGENT (El Director de Orquesta)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Input: url_analyzed events (de DASTySASTAgent, NucleiAgent, GoSpider)
‚îÇ
‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STEP 1: EVENT RECEPTION (Real-time)                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üì® Event Bus Subscription                                     ‚îÇ
‚îÇ  ‚Ä¢ Escucha: EventType.URL_ANALYZED                             ‚îÇ
‚îÇ  ‚Ä¢ Payload: {                                                   ‚îÇ
‚îÇ      "url": "https://example.com/product?id=123",              ‚îÇ
‚îÇ      "vulnerabilities": [                                       ‚îÇ
‚îÇ        {                                                        ‚îÇ
‚îÇ          "type": "XSS",                                         ‚îÇ
‚îÇ          "parameter": "q",                                      ‚îÇ
‚îÇ          "confidence": 0.8,                                     ‚îÇ
‚îÇ          "severity": "high",                                    ‚îÇ
‚îÇ          "skeptical_score": 7,                                  ‚îÇ
‚îÇ          "evidence": {...}                                      ‚îÇ
‚îÇ        },                                                       ‚îÇ
‚îÇ        ...                                                      ‚îÇ
‚îÇ      ]                                                          ‚îÇ
‚îÇ    }                                                            ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚Ä¢ Puede recibir ~100 events/minuto en scans grandes           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STEP 2: DEDUPLICATION (LRU Cache)                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üîë Deduplication Key Generation                               ‚îÇ
‚îÇ  Formula: vuln_type:parameter:url_path                         ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Ejemplos:                                                      ‚îÇ
‚îÇ  ‚Ä¢ https://shop.com/product?id=1   ‚Üí "XSS:id:/product"         ‚îÇ
‚îÇ  ‚Ä¢ https://shop.com/product?id=999 ‚Üí "XSS:id:/product" (DUPE!) ‚îÇ
‚îÇ  ‚Ä¢ https://shop.com/search?q=test  ‚Üí "XSS:q:/search"           ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  LRU Cache (max_size: 1000):                                   ‚îÇ
‚îÇ  ‚Ä¢ Si key ya existe ‚Üí DUPLICATE (skip)                         ‚îÇ
‚îÇ  ‚Ä¢ Si key nueva ‚Üí A√±adir a cache                               ‚îÇ
‚îÇ  ‚Ä¢ Si cache lleno ‚Üí Evict oldest entry                         ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  M√©tricas:                                                      ‚îÇ
‚îÇ  ‚Ä¢ Total findings: 5000                                        ‚îÇ
‚îÇ  ‚Ä¢ Duplicates detected: 4500 (90%)                             ‚îÇ
‚îÇ  ‚Ä¢ Unique findings: 500                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ (~90% de findings descartados aqu√≠)
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STEP 3: FALSE POSITIVE FILTERING                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üö´ FP Confidence Threshold (default: 0.5)                     ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  IF fp_confidence < 0.5:                                       ‚îÇ
‚îÇ    IF vuln_type == "SQLi":                                     ‚îÇ
‚îÇ      ‚Üí BYPASS filter (SQLMap is authoritative)                 ‚îÇ
‚îÇ    ELSE IF probe_validated == True:                            ‚îÇ
‚îÇ      ‚Üí BYPASS filter (has concrete evidence)                   ‚îÇ
‚îÇ    ELSE:                                                        ‚îÇ
‚îÇ      ‚Üí FILTERED (too many false positives)                     ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  C√≥digo:                                                        ‚îÇ
‚îÇ  ```python                                                      ‚îÇ
‚îÇ  if not is_sqli and not probe_validated and fp_confidence < 0.5: ‚îÇ
‚îÇ      logger.info(f"Finding filtered by FP threshold")          ‚îÇ
‚îÇ      return  # DESCARTADO                                      ‚îÇ
‚îÇ  ```                                                            ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  M√©tricas:                                                      ‚îÇ
‚îÇ  ‚Ä¢ Findings after dedup: 500                                   ‚îÇ
‚îÇ  ‚Ä¢ Findings filtered by FP: 200 (40%)                          ‚îÇ
‚îÇ  ‚Ä¢ Findings remaining: 300                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ (~60% de findings sobreviven el filtro)
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STEP 4: CLASSIFICATION (Semantic Routing)                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üè∑Ô∏è Vuln Type ‚Üí Specialist Mapping                            ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  VULN_TYPE_TO_SPECIALIST = {                                   ‚îÇ
‚îÇ    # XSS variants                                              ‚îÇ
‚îÇ    "xss": "xss",                                               ‚îÇ
‚îÇ    "cross-site scripting": "xss",                              ‚îÇ
‚îÇ    "reflected xss": "xss",                                     ‚îÇ
‚îÇ    "dom xss": "xss",                                           ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ    # SQLi variants                                             ‚îÇ
‚îÇ    "sql injection": "sqli",                                    ‚îÇ
‚îÇ    "sqli": "sqli",                                             ‚îÇ
‚îÇ    "blind sqli": "sqli",                                       ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ    # SSRF variants                                             ‚îÇ
‚îÇ    "ssrf": "ssrf",                                             ‚îÇ
‚îÇ    "server-side request forgery": "ssrf",                      ‚îÇ
‚îÇ    "url injection": "ssrf",                                    ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ    # ... 60+ mappings total                                    ‚îÇ
‚îÇ  }                                                              ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Normalizaci√≥n:                                                 ‚îÇ
‚îÇ  ‚Ä¢ "Cross-Site Scripting" ‚Üí normalize ‚Üí "xss" ‚Üí XSSAgent      ‚îÇ
‚îÇ  ‚Ä¢ "SQL Injection (Blind)" ‚Üí normalize ‚Üí "sqli" ‚Üí SQLiAgent   ‚îÇ
‚îÇ  ‚Ä¢ Unknown types ‚Üí default to "generic" queue                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STEP 5: PRIORITIZATION (Weighted Scoring)                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üìä Priority Score Formula                                     ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Priority = 40% severity + 35% fp_confidence + 25% skeptical   ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Component Scores:                                              ‚îÇ
‚îÇ  ‚Ä¢ Severity (0-40 points):                                     ‚îÇ
‚îÇ    - CRITICAL ‚Üí 40                                             ‚îÇ
‚îÇ    - HIGH ‚Üí 30                                                 ‚îÇ
‚îÇ    - MEDIUM ‚Üí 20                                               ‚îÇ
‚îÇ    - LOW ‚Üí 10                                                  ‚îÇ
‚îÇ    - INFO ‚Üí 5                                                  ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚Ä¢ FP Confidence (0-35 points):                                ‚îÇ
‚îÇ    - confidence * 35                                           ‚îÇ
‚îÇ    - 1.0 confidence ‚Üí 35 points                                ‚îÇ
‚îÇ    - 0.5 confidence ‚Üí 17.5 points                              ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚Ä¢ Skeptical Score (0-25 points):                              ‚îÇ
‚îÇ    - (skeptical_score / 10) * 25                               ‚îÇ
‚îÇ    - 10 skeptical ‚Üí 25 points                                  ‚îÇ
‚îÇ    - 5 skeptical ‚Üí 12.5 points                                 ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Ejemplo:                                                       ‚îÇ
‚îÇ  Finding: {                                                     ‚îÇ
‚îÇ    severity: "high",        # 30 points                        ‚îÇ
‚îÇ    fp_confidence: 0.8,      # 28 points (0.8 * 35)             ‚îÇ
‚îÇ    skeptical_score: 7       # 17.5 points (7/10 * 25)          ‚îÇ
‚îÇ  }                                                              ‚îÇ
‚îÇ  ‚Üí Priority = 30 + 28 + 17.5 = 75.5/100 (ALTA PRIORIDAD)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STEP 6: BATCH PROCESSING (Optimization)                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üì¶ Batch Mode (optional, configurable)                        ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Config:                                                        ‚îÇ
‚îÇ  ‚Ä¢ batch_size: 10 (default)                                    ‚îÇ
‚îÇ  ‚Ä¢ batch_timeout: 30s                                          ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Behavior:                                                      ‚îÇ
‚îÇ  ‚Ä¢ Acumula findings hasta batch_size O timeout                 ‚îÇ
‚îÇ  ‚Ä¢ Procesa batch completo de golpe                             ‚îÇ
‚îÇ  ‚Ä¢ Reduce LLM API calls (bulk classification)                  ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Modo Batch OFF (default):                                     ‚îÇ
‚îÇ  ‚Ä¢ Procesa cada finding inmediatamente                         ‚îÇ
‚îÇ  ‚Ä¢ Latencia m√°s baja                                           ‚îÇ
‚îÇ  ‚Ä¢ M√°s LLM calls pero m√°s responsive                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STEP 7: QUEUE DISTRIBUTION (Specialist Routing)                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üöÄ Event Emission to Specialist Queues                        ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Specialist Queues:                                             ‚îÇ
‚îÇ  ‚Ä¢ work_queued_xss       ‚Üí XSSAgent                            ‚îÇ
‚îÇ  ‚Ä¢ work_queued_sqli      ‚Üí SQLiAgent                           ‚îÇ
‚îÇ  ‚Ä¢ work_queued_ssrf      ‚Üí SSRFAgent                           ‚îÇ
‚îÇ  ‚Ä¢ work_queued_lfi       ‚Üí LFIAgent                            ‚îÇ
‚îÇ  ‚Ä¢ work_queued_rce       ‚Üí RCEAgent                            ‚îÇ
‚îÇ  ‚Ä¢ work_queued_xxe       ‚Üí XXEAgent                            ‚îÇ
‚îÇ  ‚Ä¢ work_queued_idor      ‚Üí IDORAgent                           ‚îÇ
‚îÇ  ‚Ä¢ work_queued_jwt       ‚Üí JWTAgent                            ‚îÇ
‚îÇ  ‚Ä¢ work_queued_openredirect ‚Üí OpenRedirectAgent                ‚îÇ
‚îÇ  ‚Ä¢ work_queued_csti      ‚Üí CSTIAgent                           ‚îÇ
‚îÇ  ‚Ä¢ work_queued_prototype ‚Üí PrototypePollutionAgent             ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Event Payload:                                                 ‚îÇ
‚îÇ  {                                                              ‚îÇ
‚îÇ    "finding": {...},                                           ‚îÇ
‚îÇ    "priority": 75.5,                                           ‚îÇ
‚îÇ    "scan_context": "scan_abc123",                              ‚îÇ
‚îÇ    "classified_at": 1738435678.123                             ‚îÇ
‚îÇ  }                                                              ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Specialists consumen de su cola y atacan en paralelo          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Deduplication Logic (El Algoritmo Cr√≠tico)

### Deduplication Key Formula

```python
def _make_key(self, finding: Dict[str, Any]) -> str:
    """
    Genera clave de deduplicaci√≥n √∫nica.
    
    Format: vuln_type:parameter:url_path
    
    Ejemplos:
    - XSS en ?id= en /product ‚Üí "XSS:id:/product"
    - SQLi en ?user= en /api/users ‚Üí "SQLi:user:/api/users"
    """
    
    vuln_type = finding.get("type", "UNKNOWN").upper()
    parameter = finding.get("parameter", "UNKNOWN")
    
    # Extraer URL path (sin query string, sin dominio)
    url = finding.get("url", "")
    parsed = urlparse(url)
    url_path = parsed.path or "/"
    
    # Normalizar path (remover trailing slash, IDs numericos)
    url_path = re.sub(r'/\d+', '/{id}', url_path)  # /users/123 ‚Üí /users/{id}
    url_path = url_path.rstrip('/')
    
    key = f"{vuln_type}:{parameter}:{url_path}"
    
    return key
```

### Path Normalization (Clave para Dedup Agresivo)

```python
# ANTES de normalizaci√≥n:
"/users/1"     ‚Üí "XSS:id:/users/1"
"/users/2"     ‚Üí "XSS:id:/users/2"
"/users/999"   ‚Üí "XSS:id:/users/999"
‚Üí 3 findings DUPLICADOS SEM√ÅNTICAMENTE pero con keys diferentes

# DESPU√âS de normalizaci√≥n:
"/users/1"     ‚Üí "XSS:id:/users/{id}"
"/users/2"     ‚Üí "XSS:id:/users/{id}"  ‚Üê DUPLICATE (misma key)
"/users/999"   ‚Üí "XSS:id:/users/{id}"  ‚Üê DUPLICATE (misma key)
‚Üí Solo 1 finding √∫nico, los otros 2 descartados
```

### LRU Cache Implementation

```python
class DeduplicationCache:
    """
    LRU cache con max_size = 1000.
    
    Cuando el cache se llena:
    1. Ordenar entries por timestamp (m√°s antiguo primero)
    2. Evict oldest 10% (100 entries)
    3. A√±adir nueva entry
    """
    
    def __init__(self, max_size: int = 1000):
        self.cache: Dict[str, FindingRecord] = {}
        self.max_size = max_size
        self.hits = 0
        self.misses = 0
        self._lock = threading.Lock()
    
    def check_and_add(self, finding: Dict, scan_context: str):
        """
        Atomic check-and-add operation.
        
        Returns:
            (is_duplicate, key)
        """
        with self._lock:
            key = self._make_key(finding)
            
            # Check for duplicate
            if key in self.cache:
                self.hits += 1
                existing = self.cache[key]
                
                # Log duplicate
                logger.debug(
                    f"DUPLICATE finding: {key} "
                    f"(original from {existing.scan_context}, "
                    f"duplicate from {scan_context})"
                )
                
                # Update dedup metrics
                dedup_metrics.record_duplicate(key, scan_context)
                
                return (True, key)  # IS DUPLICATE
            
            # Not duplicate - add to cache
            self.misses += 1
            
            # Evict if cache full
            if len(self.cache) >= self.max_size:
                self._evict_oldest()
            
            # Add new entry
            self.cache[key] = FindingRecord(
                key=key,
                finding=finding,
                scan_context=scan_context
            )
            
            # Update dedup metrics
            dedup_metrics.record_unique(key, scan_context)
            
            return (False, key)  # NOT DUPLICATE
    
    def _evict_oldest(self):
        """Evict oldest 10% of entries."""
        evict_count = max(1, self.max_size // 10)
        
        # Sort by timestamp
        sorted_entries = sorted(
            self.cache.items(),
            key=lambda x: x[1].received_at
        )
        
        # Evict oldest
        for key, _ in sorted_entries[:evict_count]:
            del self.cache[key]
        
        logger.debug(f"Evicted {evict_count} oldest cache entries")
```

---

## False Positive Filtering

### The SQLi Exception Rule

**Por qu√© SQLi bypasea el filtro de FP?**

```python
# findings_consolidation_agent.py (l√≠nea ~420)

fp_confidence = finding.get("fp_confidence", 0.5)
probe_validated = finding.get("probe_validated", False)
is_sqli = "sql" in finding.get("type", "").lower()

# Standard FP filter
if not is_sqli and not probe_validated and fp_confidence < 0.5:
    logger.info(f"Finding filtered by FP threshold: {finding['id']}")
    return  # FILTERED

# SQLi bypass
if is_sqli and fp_confidence < 0.5:
    logger.info("SQLi bypass: forwarded to SQLMap for authoritative validation")
    # BYPASEA EL FILTRO - SQLMap decide, no el LLM
```

**Raz√≥n**: SQLMap es **authoritative** y **determin√≠stico**. Un LLM puede equivocarse al analizar si un par√°metro es vulnerable a SQLi, pero SQLMap ejecuta payloads reales y confirma de forma definitiva. Es mejor enviar 10 falsos positivos a SQLMap (que los rechaza en 10s) que perder 1 SQLi real.

### Probe Validated Exception

```python
# Si el probe ACTIVO confirm√≥ comportamiento sospechoso:
if probe_validated == True:
    # BYPASS el filtro de FP
    # La evidencia concreta supera al score del LLM
```

**Ejemplo**:
```json
{
  "type": "XSS",
  "parameter": "q",
  "fp_confidence": 0.3,  // Bajo (normalmente filtrado)
  "probe_validated": true,  // PERO probe confirm√≥ reflexi√≥n
  "evidence": {
    "reflection_context": "html_text",
    "survived_chars": ["<",">","\""]  // Caracteres no filtrados
  }
}
// ‚Üí NO FILTRADO (evidencia concreta > LLM score)
```

---

## Priority Scoring Formula

### Weighted Components

| Component | Weight | Range | Example |
|-----------|--------|-------|---------|
| **Severity** | 40% | 0-40 | HIGH (high=30) |
| **FP Confidence** | 35% | 0-35 | 0.8 √ó 35 = 28 |
| **Skeptical Score** | 25% | 0-25 | 7/10 √ó 25 = 17.5 |

**Total**: 0-100 points

### Severity Mapping

```python
SEVERITY_PRIORITY = {
    "critical": 40,  # RCE, SQLi con admin access
    "high": 30,      # XSS, SQLi sin admin
    "medium": 20,    # IDOR, SSRF
    "low": 10,       # Info disclosure
    "info": 5,       # Missing headers
}
```

### Examples

**Ejemplo 1: Critical SQLi con alta confianza**
```python
{
  "severity": "critical",     # 40 points
  "fp_confidence": 0.95,      # 33.25 points (0.95 * 35)
  "skeptical_score": 9        # 22.5 points (9/10 * 25)
}
# ‚Üí Priority = 40 + 33.25 + 22.5 = 95.75/100 (M√ÅXIMA PRIORIDAD)
```

**Ejemplo 2: Medium IDOR con baja confianza**
```python
{
  "severity": "medium",       # 20 points
  "fp_confidence": 0.4,       # 14 points (0.4 * 35)
  "skeptical_score": 3        # 7.5 points (3/10 * 25)
}
# ‚Üí Priority = 20 + 14 + 7.5 = 41.5/100 (BAJA PRIORIDAD)
```

**Ejemplo 3: Low info con alta confianza**
```python
{
  "severity": "low",          # 10 points
  "fp_confidence": 1.0,       # 35 points (1.0 * 35)
  "skeptical_score": 10       # 25 points (10/10 * 25)
}
# ‚Üí Priority = 10 + 35 + 25 = 70/100 (MEDIA-ALTA PRIORIDAD)
# Nota: Aunque es LOW severity, la alta confidence lo hace relevante
```

---

## Batch Processing vs Real-Time

### Real-Time Mode (Default)

```python
# config.yaml
consolidation:
  batch_mode: false  # Process findings immediately
```

**Pros**:
- ‚úÖ Latencia ultra-baja (~100ms)
- ‚úÖ Findings llegan a specialists ASAP
- ‚úÖ Better para scans interactivos

**Cons**:
- ‚ùå M√°s LLM API calls (1 call por finding)
- ‚ùå Mayor costo en scans grandes

### Batch Mode

```python
# config.yaml
consolidation:
  batch_mode: true
  batch_size: 10
  batch_timeout: 30  # seconds
```

**Pros**:
- ‚úÖ Reduce LLM calls (1 call por 10 findings)
- ‚úÖ M√°s eficiente en scans grandes
- ‚úÖ Menor costo de API

**Cons**:
- ‚ùå Latencia m√°s alta (hasta 30s de espera)
- ‚ùå Findings se acumulan en buffer

### Batch Processing Logic

```python
async def _batch_processor_loop(self):
    """
    Background task que procesa batches cada N segundos.
    """
    while self.running:
        await asyncio.sleep(self.batch_timeout)
        
        # Check if batch accumulated
        if len(self.batch_buffer) > 0:
            logger.info(f"Processing batch of {len(self.batch_buffer)} findings")
            await self.flush_batch()
```

---

## Configuraci√≥n

```yaml
consolidation:
  # Deduplication
  dedup_enabled: true
  dedup_cache_size: 1000              # LRU cache max entries
  
  # False Positive Filtering
  fp_filtering_enabled: true
  fp_confidence_threshold: 0.5        # M√≠nimo confidence para pasar
  sqli_bypass_fp_filter: true         # SQLi siempre pasa a SQLMap
  probe_bypass_fp_filter: true        # Probe validated bypasea filtro
  
  # Priority Scoring
  priority_weights:
    severity: 0.40
    fp_confidence: 0.35
    skeptical_score: 0.25
  
  # Batch Processing
  batch_mode: false                   # true = accumulate, false = immediate
  batch_size: 10
  batch_timeout: 30                   # seconds
  
  # Queue Distribution
  specialist_queues_enabled: true
  default_queue: "generic"            # For unknown vuln types
  
  # Logging
  log_duplicates: true
  log_filtered: true
  log_priority_scores: true
```

---

## M√©tricas y Reporting

### Deduplication Metrics

```python
# Al final del scan:
summary = get_dedup_summary()

{
  "total_findings_received": 5000,
  "unique_findings": 500,
  "duplicates_detected": 4500,
  "dedup_rate": 0.90,  # 90% duplicados
  
  "top_duplicated_keys": [
    {"key": "XSS:id:/product", "count": 450},
    {"key": "SQLi:user_id:/api/users", "count": 380},
    {"key": "IDOR:id:/profile", "count": 290}
  ],
  
  "cache_stats": {
    "size": 500,
    "max_size": 1000,
    "hit_rate": 0.90,
    "evictions": 0
  }
}
```

### FP Filtering Metrics

```json
{
  "findings_after_dedup": 500,
  "findings_filtered_by_fp": 200,
  "findings_passed": 300,
  "filter_rate": 0.40,
  
  "sqli_bypass_count": 15,
  "probe_bypass_count": 35
}
```

### Priority Distribution

```json
{
  "high_priority": 80,      // 70-100 score
  "medium_priority": 150,   // 40-69 score
  "low_priority": 70        // 0-39 score
}
```

---

## Ventajas del Dise√±o

‚úÖ **Deduplicaci√≥n agresiva** (90% reduction)  
‚úÖ **Filtro de FP inteligente** con excepciones (SQLi, probes)  
‚úÖ **Priorizaci√≥n weighted** multi-factor  
‚úÖ **Event-driven** (reactive, no polling)  
‚úÖ **LRU cache** con eviction autom√°tico  
‚úÖ **Thread-safe** (locks en operaciones cr√≠ticas)  

---

## Referencias

- **Event-Driven Architecture**: Reactor Pattern
- **LRU Cache**: https://en.wikipedia.org/wiki/Cache_replacement_policies#LRU
- **Weighted Scoring**: Multi-criteria decision analysis

---

*√öltima actualizaci√≥n: 2026-02-01*  
*Versi√≥n: 2.0.0 (Phoenix Edition)*
